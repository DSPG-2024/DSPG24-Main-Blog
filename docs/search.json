[
  {
    "objectID": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-community-capitals-1/Weekly_Team_Blog.html",
    "href": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-community-capitals-1/Weekly_Team_Blog.html",
    "title": "Community Capitals Week Two",
    "section": "",
    "text": "print(\"First some R Code\")\n\n[1] \"First some R Code\"\n# Check for virtual environment, if it doesn't exist, create the venv, if it exists\n# then use the venv called \"r-python\"\n\nif (virtualenv_exists(\"r-python\")) {\n  use_virtualenv(\"r-python\")\n} else {\n  virtualenv_create(\"r-python\")\n  use_virtualenv(\"r-python\")\n}\n# Installing dependencies for the python virtualenv\n\nvirtualenv_install(envname = \"r-python\", packages = c(\"numpy\", \"pandas\", \"scikit-learn\"), all = TRUE)\n\nUsing virtual environment \"r-python\" ...\n\n\n+ \"C:/Users/harun/Documents/.virtualenvs/r-python/Scripts/python.exe\" -m pip install --upgrade --no-user numpy pandas scikit-learn\nimport pandas as pd\nprint(\"Currently using Pandas Version:\", pd.__version__)\n\nCurrently using Pandas Version: 2.2.2"
  },
  {
    "objectID": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-community-capitals-1/Weekly_Team_Blog.html#event-database-continuation",
    "href": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-community-capitals-1/Weekly_Team_Blog.html#event-database-continuation",
    "title": "Community Capitals Week Two",
    "section": "Event Database Continuation",
    "text": "Event Database Continuation\nThis week we continued our work looking at datasets for different events regarding health facilities, employment shocks and disasters. Our work consisted of cleaning the datasets, combining several datasets, and creating a event_magnitude unit value that can summarize each event."
  },
  {
    "objectID": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-community-capitals-1/Weekly_Team_Blog.html#fema-disaster",
    "href": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-community-capitals-1/Weekly_Team_Blog.html#fema-disaster",
    "title": "Community Capitals Week Two",
    "section": "FEMA Disaster",
    "text": "FEMA Disaster"
  },
  {
    "objectID": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-community-capitals-1/Weekly_Team_Blog.html#employment-shocks",
    "href": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-community-capitals-1/Weekly_Team_Blog.html#employment-shocks",
    "title": "Community Capitals Week Two",
    "section": "Employment Shocks",
    "text": "Employment Shocks"
  },
  {
    "objectID": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-community-capitals-1/Weekly_Team_Blog.html#health-facilities",
    "href": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-community-capitals-1/Weekly_Team_Blog.html#health-facilities",
    "title": "Community Capitals Week Two",
    "section": "Health Facilities",
    "text": "Health Facilities"
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-community-capitals-1/Weekly_Team_Blog.html",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-community-capitals-1/Weekly_Team_Blog.html",
    "title": "Community Capitals Week One",
    "section": "",
    "text": "We are using R as our main programming language for complicated data analysis steps, so we have completed courses in terms of data manipulation, visualization, exploration and data processing.\n\n\n\nElephant\n\n\nEven though our project deals with much more complicated datasets and those courses may not be sufficient for all data analysis procedures, they are essential fundamentals for us to move on effectively."
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-community-capitals-1/Weekly_Team_Blog.html#datacamp-training",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-community-capitals-1/Weekly_Team_Blog.html#datacamp-training",
    "title": "Community Capitals Week One",
    "section": "",
    "text": "We are using R as our main programming language for complicated data analysis steps, so we have completed courses in terms of data manipulation, visualization, exploration and data processing.\n\n\n\nElephant\n\n\nEven though our project deals with much more complicated datasets and those courses may not be sufficient for all data analysis procedures, they are essential fundamentals for us to move on effectively."
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-community-capitals-1/Weekly_Team_Blog.html#community-capitals-project-introduction",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-community-capitals-1/Weekly_Team_Blog.html#community-capitals-project-introduction",
    "title": "Community Capitals Week One",
    "section": "Community Capitals Project Introduction",
    "text": "Community Capitals Project Introduction\n\nWhat is a Community and what is a Capital?\nCommunity: A healthy community has all three below\n\nGeography Based(cities, counties, neighborhoods)\nOrganized Based(school, work)\nIdentity Based(Hometown, vacation home)\n\nCapital: An asset that is “invested” to create resources.\n\n\nTypes of Community Capitals\n\nNatural Capital - The quality and quantity of natural and environmental resources existing in a community.\nCultural Capital - The values, norms, beliefs and traditions that people inherit from the family, school and community. Also includes material goods produced at a specific time and place (such as paintings, books) that have historical or cultural significance.\nHuman Capital - Attributes of individuals that provide them with the ability to earn a living, strengthen community, and otherwise contribute to community organizations, to their families, and to self- improvement (Flora et al. 2004). It includes access to education and knowledge development, training and skill building activities and efforts to build and expand local leadership.\nSocial Capital - Connections existing among people and organizations that help make things happen in the community. Includes close ties that build community cohesion (bonding) as well as weaker ties with local and outside people and organizations that help promote broad-based action on key matters (bridging).\nPolitical Capital - The ability to influence and enforce rules, regulations, and standards. Access to individuals and groups with the power to influence decisions. Participating in civic discourse on difficult public issues.\nFinancial Capital - The variety of financial resources available to invest in local projects or economic development initiatives. Efforts to build wealth to support community development activities.\nBuilt Capital - Represents the infrastructure of the community – the basic set of facilities, services and physical structures needed by a community.\n\n\n\nProject Goal\nCurrently, we are seeking measurements that might influence these forms of capital and reliable data sources for them. For example, a measurement could be the number of businesses per county, which could significantly impact financial capital.\nPrevious projects have identified numerous measures for capitals such as Built, Financial, and Natural, but have found limited measures for Social and Cultural capital. This is because social and cultural aspects are challenging to quantify and collect data on, unlike Financial capital, which is inherently numerical.\nGoal in the future, gather diverse measurements and data sets for each type of capital, we aim to develop an index value representing each capital for each county in Iowa to visualize."
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-community-capitals-1/Weekly_Team_Blog.html#social-cultural-capitals",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-community-capitals-1/Weekly_Team_Blog.html#social-cultural-capitals",
    "title": "Community Capitals Week One",
    "section": "Social & Cultural Capitals",
    "text": "Social & Cultural Capitals\n\nBeginning\nThroughout this week we have been brainstorming ways that we can measure Social and Cultural Capital. We read articles and looked at websites to gain knowledge on potential measures. As we started our research, we wrote down measure ideas along the way. We discussed some of our ideas and talked about our concerns of the project so far. Our biggest concern was finding cultural measures.\n\n\nSocial Capitals:\n\nVolunteering rates\nParticipation in Civic Acts/Election turnout\nParticipation frequency in Clubs/ Communities\nCrime rates\nNumber of visits to the public library and the programs\nDonations to local candidate committees and ballot issue committees\n\n\n\nCultural Capitals:\n\nArt festival/ cultural events turnout\nMembership of cultural organizations\nNumber of distinct religions represented\nFrequency of attendance at religion services\n\n\n\nFindings\nWe found that it is much easier to think of social capitals compared to cultural capitals. The tricky part about Cultural Capitals is being able to find datasets for our measures. For example, a community could have a lot of cultural capital due to holding cultural events but there is very little or no data for us to look at.\nSome of the sources that we have found so far are:\n\nAmeriCorps\n\nInformation about volunteering and civic life\n\nSocial Capital Atlas\n\nInformation about economic connectedness, cohesiveness, and civic engagement\n\nIowa Campaign Contributions Received\n\nInformation about political campaigns (committee, contribution amount, city)\n\nIowa Public Library Statistics\n\nInformation about the public libraries in Iowa counties"
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-community-capitals-1/Weekly_Team_Blog.html#events-database",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-community-capitals-1/Weekly_Team_Blog.html#events-database",
    "title": "Community Capitals Week One",
    "section": "Events Database",
    "text": "Events Database\n\nFEMA Dataset\nThe Federal Management Agency(FEMA) disasters dataset includes various disasters/events that have taken place throughout the United States over the years. These datasets also include all the grants and funding approved by various organizations.\n\nArea Dataset\n\n# import pandas as pd\n# areas_df = pd.read_csv(\"Data/Updated_FEMA_Disaster_Areas.csv\")\n# areas_df\n\nArea Dataset Feature Selection:\n\nid: Not needed\ndisasterNumber: Might be important, will keep it for now.\nprogramTypeCode: Redundant, this info is available in programTypeDescription\nprogramTypeDescription: Will keep\nstateCode: Will either keep this or stateName but will drop one of them\nstateNames: Same as stateCode\nplaceCode: Keep for now\nplaceName: Most seems to be based on county but need to get rid of the ‘(county)’ part.\ndesignatedDate: Convert to Date type and clean up\nentryDate: Convert to Date type and clean up\nupdateDate: Convert to Date type and clean up\ncloseoutDate: Convert to Date type and clean up\nhash: not needed\nlastRefresh: Not important\n\n\n\nAmounts & Declarations Dataset\nSimilar feature selection process was applied to these datasets as well.\nAmounts Dataset\n\n# import pandas as pd\n# amounts_df = pd.read_csv(\"Data/FEMA_Disaster_Amounts.csv\")\n# amounts_df\n\nDeclarations Dataset\n\n# dec_df = pd.read_csv(\"Data/FEMA_Disaster_Declarations.csv\")\n\n\n#Dropping mostly dates, state and county code which are available in the Areas dataset\n# decColumnsToDrop = dec_df.columns[[0,2,3,4,5,7,12,13,14,16,17,20,21,22,23,24]]\n# dec_df = dec_df.drop(columns=decColumnsToDrop)\n# dec_df\n\nGoal next week:\nSince all three datasets include the column ‘districtNumber’ that match with each other, I plan to join these datasets into a new datasets which will follow a similar format.\n\n\n\n\nEmployment Shocks\nWe will be working with the Employment Shocks Data. The data is from workforce iowa.gov and LayoffData.com. The purpose of looking at employment shocks can help explain a communities well being. For example in the Tyson Food plants closure in Parry, Iowa will have a huge impact on it’s community. 1,276 people will be losing their job. As we started to look the datasets we found that there are three different notice types: Amendment, Mass Layoff, Closing, and Workforce reduction. we found that there were some miss spellings or slight character differences between the name of the companies and cities. Her next steps are to continue cleaning the data, fix the values so that they match up with each other, and start making visualizations.\n\n# Read in the data set\nWARN_2005_2024 &lt;- read.csv(\"Data/WARN_2005_2024.csv\")\n# WARN_2005_2024$`WARN Received Date` &lt;- as.POSIXct(WARN_2005_2024$`WARN Received Date`, format = \"%m/%d/%Y\")\n# WARN_2005_2024$`Effective Date` &lt;- as.POSIXct(WARN_2005_2024$`Effective Date`, format = \"%m/%d/%Y\")\n\n# Variables\nW05_24_V &lt;- WARN_2005_2024[ ,-c(8,9,10,13)]\n\n\n# Unique values\nsort(unique(WARN_2005_2024$`Company`))\nsort(unique(WARN_2005_2024$`City`))\nunique(WARN_2005_2024$`Notice Type`)\nsort(unique(WARN_2005_2024$`County`))\n\n\n\n\nHealth\nWe believe that there are some indirect correlations between health facilities and social capitals, especially the establishments and closures of health facilities in some specific areas leading to changes in capitals.\nThat’s why we collect and analyse data sources regarding nursing homes. We have 3 datasets:\n\nNursing Homes from CMS, consisting of 100 columns and 14,860 observations\nIowa Medicaid Payments with 9 columns and 98,305 observations representing the Medicaid reports\nNursing Facility Closures includes data for facilities that closed in 2023\n\nand 9 datasets for nursing homes from 2015 to 2022, which will be used to identify the facility closures.\nIllustrations:\n\ncms_nursinghome_df = read.csv(\"Data/CMS_Nursing_Homes.csv\")\ncolnames(cms_nursinghome_df) &lt;- gsub(\"\\\\.\", \" \", colnames(cms_nursinghome_df))\nhead(cms_nursinghome_df)\n\n\n\n\nElephant\n\n\n\nmedicaid_df = read.csv(\"Data/Iowa_Medicaid_Payments.csv\")\ncolnames(medicaid_df) &lt;- gsub(\"\\\\.\", \" \", colnames(medicaid_df))\ncolnames(medicaid_df)[colnames(medicaid_df) == \"Vendor Name\"] &lt;- \"Provider Name\"\ncolnames(medicaid_df)[colnames(medicaid_df) == \"Vendor Number\"] &lt;- \"Provider Number\"\nhead(medicaid_df)\n\n\n\n\nElephant\n\n\n\nclosure_df = readxl::read_excel(\"Data/Nursing_Facility_Closures.xlsx\")\nhead(closure_df)\n\n\n\n\nElephant\n\n\nFirst approach: We want to identify the opening and closing dates for each provider, so our first approach is to utilize the “Report Date” column in the medicaid dataset. Whenever a payment is made to a provider, a report will be recorded. So we first used the most recent report date and the oldest report date as estimates for the opening and closing dates, but…\n\nmedicaid_df_latest_filtered &lt;- medicaid_df %&gt;%\n                          group_by(`Provider Name`) %&gt;%\n                          summarise(estimate_closing = max(`Report Date`))\nmedicaid_df_oldest_filtered &lt;- medicaid_df %&gt;%\n                          group_by(`Provider Name`) %&gt;%\n                          summarise(estimate_opening = min(`Report Date`))\nopening_closing_medicaid &lt;- medicaid_df_latest_filtered %&gt;%\n                              full_join(medicaid_df_oldest_filtered, by = \"Provider Name\") %&gt;%\n                              select(`Provider Name`, `estimate_opening`, `estimate_closing`)\nopening_closing_medicaid\n\n\n\n\nElephant\n\n\nSecond approach: We collect 9 additional datasets regarding active providers from 2015 to 2022, and compare the consecutive years to estimate the year a provider closes. Illustrations are below: \nSo by looking at this planned dataframe, we can estimate the provider with ID 1506000 closed in 2018 and use that insight to identify the effect of the closure on the community."
  },
  {
    "objectID": "studentBlogs/neha/student-midpoint-blog-NehaTiru/Neha_Midpoint_Blog.html",
    "href": "studentBlogs/neha/student-midpoint-blog-NehaTiru/Neha_Midpoint_Blog.html",
    "title": "Neha’s Midpoint Reflection",
    "section": "",
    "text": "What have you been learning so far?\n\nI’m currently working on an AI & Housing project in the DSPG program, and it’s been a great learning experience.I’ve been improving my programming skills in Python, learning how to process images, and exploring data. I also started using PyTorch, which is helping me understand more about how AI works.\nAnother cool tool I’ve been using is the leaflet package. It lets us turn data into maps that you can interact with. We’re trying to combine these maps with AI to make them even more user-friendly.\nAlongside Python, I’ve been picking up some skills in R programming. This has been a new area for me, and I’ve enjoyed learning how to create blogs in R and getting to know my way around RStudio.\nI’m excited about all these new skills because they help me do more with technology and make things easier for others to understand.\n\n\n\nDemonstrate some of the work from your learning.\n\nImage Processing Activity\n\n\n\n\n\n\n\n \n\n\n \n\n\n\n\nLeaflet Map Visualization\n\n\n\nLow-Income Block Group Level map\n\n\n\nStreet level map for Low-Income Housing Tax Credit data\n\n\nSeniors Age Distributation map in Des Moines\n\n\n\nNew Directions in Learning\n\nDuring the program,I’ve been learning about new technology, which is important for my future. AI is a big part of this, and I’m excited because it’s something that can really help in many jobs today.\n\n\n\nSkills I Want to Improve\n\nFor the rest of the program, I want to get better at using Python and working with AI. I also want to be better at making maps from data. These skills will help me do my projects better and faster. I also want to work on being a good team player and communicating well with others.\n\n\n\nAm I Meeting My Goals?\n\nYes, I think I am. When I started this program, I set some goals for what I wanted to learn and do. So far, I feel like I’m on the right path and making good progress. I’m using what I learn, and I’m excited to keep going and learn more."
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-housing-ai/Weekly_Team_Blog.html",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-housing-ai/Weekly_Team_Blog.html",
    "title": "Housing and AI Week One",
    "section": "",
    "text": "DataCamp has been an incredible platform for acquiring new skills. Recently, we’ve made significant progress in learning about image processing and deep learning using Python and PyTorch—areas we were eager to explore.\nIn the Image Processing Chapter, I’ve delved into several powerful techniques:\n\nGlobal Thresholding\nEdge Detection\nTransformations\nImage Restoration\nSuperpixel & Segmentation\nFinding Contours\nMorphology, etc\n\nTurning to PyTorch, I’ve really broadened my set of practical skills in:\n\nCreating tensors from Numpy arrays\nThe Sigmoid and Softmax functions\nCalculating cross entropy loss\nUsing the MSELoss\nImplementing leaky Relu\nCalculating accuracy using Torchmetrics\nBuilding a forecasting RNN\nLSTM and GRU network, and lot more!\n\nOur learning experience with DataCamp has been incredibly rewarding, giving us the chance to deepening our understanding of the technologies."
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-housing-ai/Weekly_Team_Blog.html#datacamp",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-housing-ai/Weekly_Team_Blog.html#datacamp",
    "title": "Housing and AI Week One",
    "section": "",
    "text": "DataCamp has been an incredible platform for acquiring new skills. Recently, we’ve made significant progress in learning about image processing and deep learning using Python and PyTorch—areas we were eager to explore.\nIn the Image Processing Chapter, I’ve delved into several powerful techniques:\n\nGlobal Thresholding\nEdge Detection\nTransformations\nImage Restoration\nSuperpixel & Segmentation\nFinding Contours\nMorphology, etc\n\nTurning to PyTorch, I’ve really broadened my set of practical skills in:\n\nCreating tensors from Numpy arrays\nThe Sigmoid and Softmax functions\nCalculating cross entropy loss\nUsing the MSELoss\nImplementing leaky Relu\nCalculating accuracy using Torchmetrics\nBuilding a forecasting RNN\nLSTM and GRU network, and lot more!\n\nOur learning experience with DataCamp has been incredibly rewarding, giving us the chance to deepening our understanding of the technologies."
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-housing-ai/Weekly_Team_Blog.html#image-processing-activity",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-housing-ai/Weekly_Team_Blog.html#image-processing-activity",
    "title": "Housing and AI Week One",
    "section": "Image-Processing Activity:",
    "text": "Image-Processing Activity:\nBased on our Data Camp Training our Graduate lead Karthik has assigned us a Image Processing activity to work with.\nWe were given an image of a Waterloo Sanborn map to experiment with the image processing techniques we’ve learned.\n\nInitially, we converted the original RGB image into a gray-scale image. Subsequently, we applied various techniques that I learned during the Data Camp sessions. For instance, we implemented Sobel edge detection to highlight edges within the image. we also explored morphological operations to produce an eroded version of the image, which helps in understanding structural elements. Additionally, we generated a gray-scale histogram to analyze the intensity distribution and applied a global threshold to segment the image based on intensity values.\nEdge Detection:\n\nContour and Object Extracted:\n\n\nWe have provided the github link to Image-Processing Activity : https://github.com/NehaTiru/Image_Processing/blob/main/Untitled2.ipynb"
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-housing-ai/Weekly_Team_Blog.html#data-exploration",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-housing-ai/Weekly_Team_Blog.html#data-exploration",
    "title": "Housing and AI Week One",
    "section": "Data Exploration",
    "text": "Data Exploration\nFor our project we started by trying to find how small of data we could go for population data in terms of density, senior citizens, and income bracket. The idea is we want to be gather data at the smallest level we can with hopes of going down to the household level if possible. To do this we looked at two communities of varying sizes to see what level of data is available, Des Moines and Clear Lake.\nStarting with population density this week, we started off at the city level using Census data from the ACS-5 year (2022) to make sure data was available and the results are displayed in the graph below.\n\nSource: US Census ACS-5 (2022)\nFrom there we moved on to see if data was available at the block group and block level, which data was found for population but coming up with density would be a bit time intensive especially in Des Moines. Below is the population density of the block groups in Clear Lake as well as the block layout of Des Moines in which the process of getting population density would be similar yet time consuming.\nClear Lake Block Group Population Density\n Source: US Census ACS-5 (2022)\nDes Moines Block Population\n Source: Us Census 2020 Decennial Data\n\n\n#|echo = FALSE\npopulation_elderly2 &lt;- readRDS(file = \"Data/populationElderly2.RDS\")\n\n\nMoving forward we could look into remote sensing as a possibility to get even smaller data using machine learning. Similar to research that was done at the Nanjing University of Information Science and Technology for finding population density in Beijing City.\nFor More Information: https://www.mdpi.com/2072-4292/12/12/1910\n\nSources:\n\nThe demographic statistical atlas of the united states—Statistical atlas. (n.d.). Retrieved May 31, 2024, from https://statisticalatlas.com/place/Iowa/Des-Moines/Population\nHe, M., Xu, Y., & Li, N. (2020). Population spatialization in beijing city based on machine learning and multisource remote sensing data. Remote Sensing, 12(12), 1910. https://doi.org/10.3390/rs12121910\nGrid view: Table b09020—Census reporter. Retrieved May 31, 2024, from https://censusreporter.org/data/table/?table=B09020&geo_ids=16000US1921000,150%7C16000US1921000&primary_geo_id=16000US1921000#valueType%7Cestimate\nCensus tabulation detail: Total Population. Census Reporter. Retrieved May 31, 2024, from https://censusreporter.org/tables/B01003/"
  },
  {
    "objectID": "blogs2024/welcome/index.html",
    "href": "blogs2024/welcome/index.html",
    "title": "Blog Writing Guidelines",
    "section": "",
    "text": "Welcome to the DSPG Quarto blog. This post will be a demonstration of some of the capabilities of a quarto doc and some basics of dos and don’ts as you’re writing your own blogs throughout the DSPG program."
  },
  {
    "objectID": "blogs2024/welcome/index.html#rmarkdown-and-quarto-markdown",
    "href": "blogs2024/welcome/index.html#rmarkdown-and-quarto-markdown",
    "title": "Blog Writing Guidelines",
    "section": "RMarkdown and Quarto Markdown",
    "text": "RMarkdown and Quarto Markdown\nThese blogs are generated using Quarto Markdown (.qmd). Markdown is a special type of text format that is both legible with and without its rendering. Quarto Markdown follows the syntax of basic Markdown and has some additional features for use as well. The Markdown Basics guide on the Quarto page will be a great resource to view how you can format your texts."
  },
  {
    "objectID": "blogs2024/welcome/index.html#yaml-headers",
    "href": "blogs2024/welcome/index.html#yaml-headers",
    "title": "Blog Writing Guidelines",
    "section": "Yaml Headers",
    "text": "Yaml Headers\nEach blog has its own yaml style metadata section at the top of the file. This one has a metadata section that looks something like this in the source code.\n---\ntitle: \"Blog Writing Guidelines\"  # The title of your post\nauthor: \"Harun Celik\"             # The author associated with the blog post\ndate: \"2024-05-05\"                # The date in Year-Month-Day format\ncategories: [blog tutorial]       # The categories associated with the blog\n---\nThere are additional options for the yaml that you can check out through the Quarto documentation page."
  },
  {
    "objectID": "blogs2024/welcome/index.html#linking-pages-and-images",
    "href": "blogs2024/welcome/index.html#linking-pages-and-images",
    "title": "Blog Writing Guidelines",
    "section": "Linking Pages and Images",
    "text": "Linking Pages and Images\nYou can link items in your files by using the following syntax [Text of Link](Link Reference - URL). Referencing also works for adding images into your blogs. The syntax is very similar, but requires an ! in the front. So an example would be ![Stock Photo of a Data Cat](imgs/Data_Cat.png)\n\n\n\nStock Photo of a Data Cat\n\n\nSince this post doesn’t specify an explicit image in the yaml, the first image in the post will be used as the thumbnail of the post. A couple of things to remember with linking images.\n\nPlace your images in the imgs directory. This helps prevent clutter and makes it easier to find the files.\nName your images explicitly. Don’t call them \"image_1.jpg\" but instead something like \"age_scatterplot_1\".\nDon’t save file names with any spaces. Add an _ or use camelCaseNaming. Spaces will prevent the blog from properly rendering!\nMake sure you follow copyright guidelines on images. Don’t add images you would not be permitted to share in any professional environment."
  },
  {
    "objectID": "blogs2024/welcome/index.html#running-code",
    "href": "blogs2024/welcome/index.html#running-code",
    "title": "Blog Writing Guidelines",
    "section": "Running Code",
    "text": "Running Code\n\nCode Chunks\nOne of the coolest features of the Quarto blogs is the ability to integrate code chunks and have them render. Let’s check out how this works. Here is an example of what a code chunk looks like in the source code.\n\nprint(\"I would like for this text to print.\")\n\n[1] \"I would like for this text to print.\"\n\n\nUsing different markdown arguments, we can customize our code chunks to prevent them from running, displaying, or rendering messages. You can check out some of those options through the R Markdown Reference Guide.\nSome useful ones are below.\neval=TRUE      # If FALSE, knitr will not run the code in the code chunk.\ninclude=TRUE   # If FALSE, knitr will run the chunk but not include the chunk in the final document.\necho=TRUE      # If FALSE, knitr will not display the code in the code chunk above it’s results in the final document.\nmessage=TRUE   # If FALSE, knitr will not display any messages generated by the code.\nwarning=TRUE   # If FALSE, knitr will not display any warning messages generated by the code\n\n\nUsing Data\nMost of the time, you’ll want to run some code that generates a table or a visual of your data. To do this, quarto will need a local reference to your data and that’s why your blogs also come with a directory labelled data\\. If you are working within R, it is advisable to use the .RDS format for saving any data you want to use for displaying code. Let’s look at an example.\n\n# Let's create a data frame to use\n\n## Generate pay grades as factors\nGrades &lt;- c(\"Grade A\", \"Grade B\", \"Grade C\", \"Grade D\", \"Grade E\")\nGrades_Factor &lt;- factor(Grades, levels = c(\"Grade A\", \"Grade B\", \"Grade C\", \"Grade D\", \"Grade E\"))\n\n# Create randomized vectors of size 50\nGender &lt;- sample(x = c(\"Woman\", \"Man\", \"Transgender\", \"Non-binary/non-conforming\", \"Prefer not to respond\"), size=50, replace = TRUE)\nID &lt;- sample(x = 1000:3000, size=50)\nPay &lt;- sample(x = Grades_Factor, size=50, replace=TRUE)\n\n# Create data frame\ndf &lt;- data.frame(Gender, ID, Pay)\n\nIn our example here we’ve created the data in our blog environment, but often this will not be the case. We are going to pretend that our data is stored in the data directory. We will first save the data we created.\n\n# Saving our dataset\nsaveRDS(df, file = \"data/PayData.RDS\")\n\nNow we can load it in as Pay_DF.\n\nPay_DF &lt;- readRDS(\"data/PayData.RDS\")\n\nLet’s write some code with ggplot2 to create a visual.\n\nlibrary(ggplot2)\n\npay_colors &lt;- c(\"Grade A\" = \"#1f78b4\", \"Grade B\" = \"#33a02c\", \"Grade C\" = \"#e31a1c\", \"Grade D\" = \"#ff7f00\", \"Grade E\" = \"#6a3d9a\")\n\nggplot(Pay_DF, aes(x = Gender, fill = Pay)) +\n  geom_bar() +\n  labs(x = \"Gender\", y = \"Count\", title = \"Distribution of Pay Grades Across Genders\") +\n  scale_fill_manual(values = pay_colors) +\n  theme_minimal() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\nData Guidelines\nHere are some important things to consider when you’re using data sets in your blogs.\n\nVERY IMPORTANT: All data saved in the data directory is publicly visible. Don’t share data that is private into your blogs.\nBe mindful of the size of your data. The larger the data, the longer it will take to render. GitHub also limits commit sizes so keep this in mind."
  },
  {
    "objectID": "blogs2024/welcome/index.html#using-other-languages",
    "href": "blogs2024/welcome/index.html#using-other-languages",
    "title": "Blog Writing Guidelines",
    "section": "Using Other Languages",
    "text": "Using Other Languages\nThe last thing we will look at is displaying code for languages other than R. RMarkdown files have the ability to change the rendering engine so that other languages can also be used. To run Python, you will have to have the reticulate library installed and configured. When you’ve created a code chunk, you will want to switch the section that is typically in the form of {r} to {python} or whatever other supported language you wish to use.\nHere is a Python code block.\n\nimport platform\n\nprint(\"This code is run using python version\", platform.python_version())\n\nThis code is run using python version 3.9.18"
  },
  {
    "objectID": "blogs2024/welcome/index.html#resources",
    "href": "blogs2024/welcome/index.html#resources",
    "title": "Blog Writing Guidelines",
    "section": "Resources",
    "text": "Resources\n\nQuarto Tutorials\nQuarto Websites/Blogs\nQuarto Markdown Reference Guide\nRMarkdown Reference Guide"
  },
  {
    "objectID": "studentBlogs/cael/student-midpoint-blog-caelliastate/Cael_Midpoint_Blog.html",
    "href": "studentBlogs/cael/student-midpoint-blog-caelliastate/Cael_Midpoint_Blog.html",
    "title": "Cael’s Midpoint Blog",
    "section": "",
    "text": "Throughout the course of the program so far, I have learned a lot of new techniques and a lot of new databases of information. The biggest area I have learned so far is mapping and understanding geospatial data. Before starting this program I knew nothing about geospatial data and how it worked, now I am able to take this information and visualize it on a map and join it with multiple pieces of information to gain a better understanding of the layout of cities and distribution of the population. As seen in the picture below regarding the distribution of population density (per sq. km.) of Polk County at the census block group level.\n\nblock_population2 &lt;- get_acs(\n  geography = \"block group\",\n  state = \"IA\",\n  county = \"Polk\",\n  variable = \"B01003_001\",\n  year = 2022,\n  geometry = TRUE,\n  keep_geo_vars = TRUE\n)\n\nsaveRDS(block_population2, \"data/Block_Pop_DM2.RDS\")\n\n\nblock_population2 = readRDS(file = \"data/Block_Pop_DM2.RDS\")\nblock_population2$ALAND &lt;- block_population2$ALAND/1000000\nblock_population2$Density &lt;- block_population2$estimate/block_population2$ALAND\nmapview(block_population2, zcol = \"Density\", layer.name = \"Population Density (per sq. km.)\")\n\n\n\n\n\nIn addition to this I have been able to visualize geospatial data outside the US census and have been able to apply it to building level and tax parcel level data as well.\n\nAnother thing that I have learned during this program is image processing. Through image processing I was able to take and image and apply numerous features to it such as edge detection, contouring, gray scaling, and feature detection. Displayed below is those same features applied to a Sanborn map of Waterloo, Iowa.\n\n\n\n\n\n\n\n\n\nEdge Detection\n\n\n\n\n\n\n\nContouring\n\n\n\n\n\n\n\nFeature Detection\n\n\n\n\n\n\n\nGray Scaling\n\n\n\n\n\nFinally, the last thing that I’m learning is the process and communication that comes along with a real-world data science project. At the beginning of the program, one of the things I wanted to learn was how the process of a data science project looks outside of a classroom and this project is giving me valuable experience as I am learning the communication that is needed to advance the project forward and to produce the best product we can as a group. Additionally, it takes a lot of effort to explore through what data is valuable and data that isn’t needed and that ability to be patient through that process is needed cause something may not appear right away, but it will in time."
  },
  {
    "objectID": "studentBlogs/cael/student-midpoint-blog-caelliastate/Cael_Midpoint_Blog.html#what-have-you-been-learning-so-far",
    "href": "studentBlogs/cael/student-midpoint-blog-caelliastate/Cael_Midpoint_Blog.html#what-have-you-been-learning-so-far",
    "title": "Cael’s Midpoint Blog",
    "section": "",
    "text": "Throughout the course of the program so far, I have learned a lot of new techniques and a lot of new databases of information. The biggest area I have learned so far is mapping and understanding geospatial data. Before starting this program I knew nothing about geospatial data and how it worked, now I am able to take this information and visualize it on a map and join it with multiple pieces of information to gain a better understanding of the layout of cities and distribution of the population. As seen in the picture below regarding the distribution of population density (per sq. km.) of Polk County at the census block group level.\n\nblock_population2 &lt;- get_acs(\n  geography = \"block group\",\n  state = \"IA\",\n  county = \"Polk\",\n  variable = \"B01003_001\",\n  year = 2022,\n  geometry = TRUE,\n  keep_geo_vars = TRUE\n)\n\nsaveRDS(block_population2, \"data/Block_Pop_DM2.RDS\")\n\n\nblock_population2 = readRDS(file = \"data/Block_Pop_DM2.RDS\")\nblock_population2$ALAND &lt;- block_population2$ALAND/1000000\nblock_population2$Density &lt;- block_population2$estimate/block_population2$ALAND\nmapview(block_population2, zcol = \"Density\", layer.name = \"Population Density (per sq. km.)\")\n\n\n\n\n\nIn addition to this I have been able to visualize geospatial data outside the US census and have been able to apply it to building level and tax parcel level data as well.\n\nAnother thing that I have learned during this program is image processing. Through image processing I was able to take and image and apply numerous features to it such as edge detection, contouring, gray scaling, and feature detection. Displayed below is those same features applied to a Sanborn map of Waterloo, Iowa.\n\n\n\n\n\n\n\n\n\nEdge Detection\n\n\n\n\n\n\n\nContouring\n\n\n\n\n\n\n\nFeature Detection\n\n\n\n\n\n\n\nGray Scaling\n\n\n\n\n\nFinally, the last thing that I’m learning is the process and communication that comes along with a real-world data science project. At the beginning of the program, one of the things I wanted to learn was how the process of a data science project looks outside of a classroom and this project is giving me valuable experience as I am learning the communication that is needed to advance the project forward and to produce the best product we can as a group. Additionally, it takes a lot of effort to explore through what data is valuable and data that isn’t needed and that ability to be patient through that process is needed cause something may not appear right away, but it will in time."
  },
  {
    "objectID": "studentBlogs/cael/student-midpoint-blog-caelliastate/Cael_Midpoint_Blog.html#what-is-the-new-direction-of-learning-that-youve-been-pursuing-during-the-program-why",
    "href": "studentBlogs/cael/student-midpoint-blog-caelliastate/Cael_Midpoint_Blog.html#what-is-the-new-direction-of-learning-that-youve-been-pursuing-during-the-program-why",
    "title": "Cael’s Midpoint Blog",
    "section": "What is the new direction of learning that you’ve been pursuing during the program? Why?",
    "text": "What is the new direction of learning that you’ve been pursuing during the program? Why?\nThe new direction of learning that I am taking is coming in with an open mind and kinda acting like a sponge and trying soak up as much information as possible as I have been working on a lot of new areas of data science such as GIS, satellite imagery, and Prithvi segmentation modeling. Taking in this mentality allows me to broaden my understanding of the material and allow me to grow as a data scientist. Additionally, taking in help from my peers has been very helpful in advancing the project and my understanding further when I get stuck due to errors or"
  },
  {
    "objectID": "studentBlogs/cael/student-midpoint-blog-caelliastate/Cael_Midpoint_Blog.html#what-skills-are-you-hoping-to-improve-for-the-rest-of-the-program",
    "href": "studentBlogs/cael/student-midpoint-blog-caelliastate/Cael_Midpoint_Blog.html#what-skills-are-you-hoping-to-improve-for-the-rest-of-the-program",
    "title": "Cael’s Midpoint Blog",
    "section": "What skills are you hoping to improve for the rest of the program?",
    "text": "What skills are you hoping to improve for the rest of the program?\nOne skill that I’m hoping to improve through the rest of the program is working with satellite imagery data and in turn using that data for parcel detection as being able to do so with help expand our model to more rural communities without the need from pulling from every county assessor’s webpage for the information or for communities that don’t digitize that information.\nAnother skill that I’m hoping to improve is working with AI. This comes as we are soon going to be training our AI model and being able to build up that skill of working and training AI will look good to future hiring managers for future employment.\nThe last skill I hope to improve upon is my python coding as I will be having to build on the pretrained Prithvi model in python to fit our needs of parcel segmentation without the need for assessor data from every county. Although it has been a bit since I worked fully with python, I do have a solid background with it and through this model building process I hope to expand my knowledge further in understanding functions as well as machine learning in python."
  },
  {
    "objectID": "studentBlogs/cael/student-midpoint-blog-caelliastate/Cael_Midpoint_Blog.html#do-you-think-that-youre-meeting-the-goals-youve-outlined-at-the-start-of-the-program",
    "href": "studentBlogs/cael/student-midpoint-blog-caelliastate/Cael_Midpoint_Blog.html#do-you-think-that-youre-meeting-the-goals-youve-outlined-at-the-start-of-the-program",
    "title": "Cael’s Midpoint Blog",
    "section": "Do you think that you’re meeting the goals you’ve outlined at the start of the program?",
    "text": "Do you think that you’re meeting the goals you’ve outlined at the start of the program?\nI believe that I am growing towards completing the goals that I had set out for myself at the beginning of the program. For example, I have done a lot of work with GIS data which was the biggest goal I had for myself and wanted to gain a lot of experience with GIS mapping. Another goal I had was being able to learn what the data science process looks like in a professional setting and it would be an understatement to say that I have definitely achieved that goal."
  },
  {
    "objectID": "studentBlogs/nhat/student-midpoint-blog-im-anhat/Chris_Midpoint_Blog.html#what-i-learned",
    "href": "studentBlogs/nhat/student-midpoint-blog-im-anhat/Chris_Midpoint_Blog.html#what-i-learned",
    "title": "Chris’ Midpoint Reflection",
    "section": "What I learned?",
    "text": "What I learned?\n\nData Manipulation and Preprocessing in R\n\n\n\nData Processing\n\n\nI collected and worked with around 10 datasets at the same time, which enables me to handle data proficiently to make the best insights. From normalizing the features / attributes to summarizing every individual data point by an index, my data processing skills have been improved a lot.\n\n\nResults\nBefore processinng, two datasets with numerous columns about distributions of different races and ancestries within cities\n\n# Before processing, two datasets with numerous columns about distributions of\n# different races and ancestries within cities\nancestry_df &lt;- read.csv(\"./data/ancestry.csv\")\nrace_df &lt;- read.csv(\"./data/race.csv\")\n\n B04006_002E is the number of Afghan people with that city\n\n# After processing, Simpson index indicates the diversity within a city\ncultural_df &lt;- read.csv(\"./data/ancestry_race.csv\")\nhead(cultural_df, 5)\n\n    GEOID     CITY SimpsonIndex_race SimpsonIndex_ancestry\n1 1900190   Ackley        0.24674401             0.8196178\n2 1900235 Ackworth        0.17250674             0.8134410\n3 1900370    Adair        0.09779811             0.7921790\n4 1900505     Adel        0.17635778             0.8601441\n5 1900595    Afton        0.26618045             0.8464086\n\n\n\n# Similarly, the summary for social capital measures\nsocial_df &lt;- read.csv(\"./data/household_proficiency_commute.csv\")\n\n\n\n\nSocial\n\n\n\n\nNew directions…\nAfter collecting datasets for social and cultural measures, we may continue on some statistical analysis, summarising helpful insights that may be used in the interactive R Shiny dashboard.\n Besides, if time permits, we may do some modelings for our capital data, such as predicting improvemets in social capitals if some specific measures are increased.\n\n\nGoals Met?\nNot fully met, but I do learn many important skills, especially in R - a language I may never use during college if not attending DSPG. I also wish I could do some machine learning-based modeling, but let’s see…\nPersonally, I see a lot of improvements in soft skills such as presentation and English communication skills. I do also gain technical developments and helpful insights from the people I met and worked with. I may always need to remember the skills I learn today at DSPG such as data preprocessing and visualization skills. Besides, I do wish to learn more about other data skills such as data pipelines and data warehouse."
  },
  {
    "objectID": "studentBlogs/nhat/student-midpoint-blog-im-anhat/Chris_Midpoint_Blog.html#what-to-focus-next",
    "href": "studentBlogs/nhat/student-midpoint-blog-im-anhat/Chris_Midpoint_Blog.html#what-to-focus-next",
    "title": "Chris’ Midpoint Reflection",
    "section": "What to focus next?",
    "text": "What to focus next?\nI would say that I want to learn more fantastic skills to make cool visualization and helpful analysis in the field of social and cultural capitals.\n\nI also hope to use more technologies to diversify my personal skills in the field of data science and analysis. Looking back on the beginning of the DSPG, I learned a lot of analysis skills. However, there’s still a lot of things to learn and make progress."
  },
  {
    "objectID": "studentBlogs/neha/student-introductions/Neha_Introduction.html",
    "href": "studentBlogs/neha/student-introductions/Neha_Introduction.html",
    "title": "Neha’s Introduction",
    "section": "",
    "text": "Personal Introduction\n\nOrigin: I am from Andhra Pradesh, India.\nAcademic Year: I will be junior in the coming fall semester.\nDegree Pursuits: I am majoring in Computer Science and minoring in Data Science. My interest in these fields was sparked by an inspiring Computer Science professor during my high school years.\nCareer Aspirations: I aim to become a software developer at a FANG company, leveraging my degrees to build a solid foundation in technology.\nInterest in Data Science: Data science fascinates me because of its power to solve complex problems and its potential to drive innovation across various industries.\nData Science Experience: I have basic knowledge in data science, familiar with tools like R and Pandas, though I am still in the early stages of my data science education.\nHobbies and Extracurricular Activities: My free time is often spent playing badminton and cricket with friends. I am also an active participant in dance programs organized by the Indian Student Association (ISA).\n\n \n\n\nPersonal Learning Objectives for DSPG\n\nValued Skills: I feel am confortable with my foundational knowledge in Python, HTML, CSS. My ability to quickly learn and apply new tech skills is something I value about myself.\nSkills to Develop:\n\nReal-world problem-solving: Learning to navigate and address complex real-world challenges.\nClient requirement fulfillment: Understanding and meeting client expectations effectively.\nIndustry readiness: Preparing myself to seamlessly transition into a professional environment.\n\nMotivation for These Skills: These skills are essential for me to have a successful career in software development, especially in a competitive and dynamic field like tech industry.\nImprovement Strategy: To enhance these skills, I plan to engage deeply with in my project at DSPG, ensuring I have a clear understanding of the underlying principles and actively seeking feedback to refine my approach."
  },
  {
    "objectID": "studentBlogs/blake/student-midpoint-blog-blakeunderwood/Blake_Midpoint_Blog.html",
    "href": "studentBlogs/blake/student-midpoint-blog-blakeunderwood/Blake_Midpoint_Blog.html",
    "title": "Blake’s Midpoint Reflection",
    "section": "",
    "text": "Throughout my first five weeks of DSPG, I have split my work between two areas. First, my colleagues and I have been building a dataset from collected data in zoning ordinances. The goal of this dataset is to create a visual guide map of all zoning codes in Iowa.\nhere is a visual of the type of app we are building:\n\nOur zoning guide will show every area and whether it is residential or not, and after clicking on a specific district you will get plenty of information about that district and the zoning laws that apply to that district."
  },
  {
    "objectID": "studentBlogs/blake/student-midpoint-blog-blakeunderwood/Blake_Midpoint_Blog.html#iowa-zoning-guide-map",
    "href": "studentBlogs/blake/student-midpoint-blog-blakeunderwood/Blake_Midpoint_Blog.html#iowa-zoning-guide-map",
    "title": "Blake’s Midpoint Reflection",
    "section": "",
    "text": "Throughout my first five weeks of DSPG, I have split my work between two areas. First, my colleagues and I have been building a dataset from collected data in zoning ordinances. The goal of this dataset is to create a visual guide map of all zoning codes in Iowa.\nhere is a visual of the type of app we are building:\n\nOur zoning guide will show every area and whether it is residential or not, and after clicking on a specific district you will get plenty of information about that district and the zoning laws that apply to that district."
  },
  {
    "objectID": "studentBlogs/blake/student-midpoint-blog-blakeunderwood/Blake_Midpoint_Blog.html#analysis-motivation",
    "href": "studentBlogs/blake/student-midpoint-blog-blakeunderwood/Blake_Midpoint_Blog.html#analysis-motivation",
    "title": "Blake’s Midpoint Reflection",
    "section": "Analysis motivation",
    "text": "Analysis motivation\nI am also working on further analysis of the zoning data on the housing market. I am using other high quality data sources as well the zoning data together to assist me in my analysis of the housing market across Iowa.\nI am using R and Python for graphics and data processing while examining a few important metrics in such as median home value, median household income, and cost of rent from the Census Bureau website."
  },
  {
    "objectID": "studentBlogs/blake/student-midpoint-blog-blakeunderwood/Blake_Midpoint_Blog.html#housing-market",
    "href": "studentBlogs/blake/student-midpoint-blog-blakeunderwood/Blake_Midpoint_Blog.html#housing-market",
    "title": "Blake’s Midpoint Reflection",
    "section": "Housing market",
    "text": "Housing market\nAn example of a map I made as part of my analysis, is a choropleth time series map in Tableau, showing the change in income and cost of living for neighborhoods across Iowa.\nTo investigate the future of the housing market, I created a linear model to estimate these metrics up to the year 2030.\nBelow is a sample of what I’ve been working on. The Tableau workbook contains four maps: (1) household income, (2) home value, (3) rent costs, and (4) my forecast map of household incomes until the year 2030. I broke down each area to the most granular level possible, so each boundary on the maps represents a block of about a couple of thousand people each."
  },
  {
    "objectID": "studentBlogs/blake/student-midpoint-blog-blakeunderwood/Blake_Midpoint_Blog.html#future",
    "href": "studentBlogs/blake/student-midpoint-blog-blakeunderwood/Blake_Midpoint_Blog.html#future",
    "title": "Blake’s Midpoint Reflection",
    "section": "future",
    "text": "future\nOur analysis will continue to look at these types of variables but over time I will continue to add other metrics for analyzing the housing market such as calculating supply and demand for a city. I’m just getting started with this project but i’m off to a great start!"
  },
  {
    "objectID": "studentBlogs/tabassum/student-introductions/Tabassum_Introduction.html",
    "href": "studentBlogs/tabassum/student-introductions/Tabassum_Introduction.html",
    "title": "Tabassum’s Introduction",
    "section": "",
    "text": "This is my favorite part THE INTRODUCTION. The ‘Why’ of this will answer my commitment for DSPG program and gratitude to be able to spend my summer with you all bright minds.\nOn the first day, while we were having this little personality test for grouping in My Creative Type I got THE ADVENTURER"
  },
  {
    "objectID": "studentBlogs/tabassum/student-introductions/Tabassum_Introduction.html#my-identity",
    "href": "studentBlogs/tabassum/student-introductions/Tabassum_Introduction.html#my-identity",
    "title": "Tabassum’s Introduction",
    "section": "My identity",
    "text": "My identity\n\nCountry\nI, Tabassum Zaman, come from a Southasian country named “Bangladesh”. Which has got some amazing stories to tell people about itself to tell the world. But I will keep it short today to be focused on the objective of this blog.\n\n\n\n\nBangladesh with its surrounding countries\n\n\n\n\n\nA view of Dhaka city (capital of Bangladesh)\n\n\n\n\n\nAcamedics\nDepartment: Community and Regional Planning\nProgram: Masters\nInstitution: Iowa State University\nI completed my undergrad in ‘Urban and Regional Planning’ from my dream university in Bangladesh which was ‘BUET [Bangladesh University of Engineering and Technology]’. I have been always interested in the Transportation problem surrounding me, may be because that’s all I saw. But with time as I am exploring more, I am finding my new interested areas in spatial problems like, housing, accessibility. I am hoping to be focused down in a very near future, as I finished up my semester of masters program this Spring 2024.\n\n\n\nMy Undergrad Convocation from BUET in Urban and Regional Planning\n\n\n\n\nWhat got me into Data Science\nAfter completing my undergrad I had the opportunity to work in the real world for more than a couple of years, in transportation, health care and commodity industry back home. There I got to know the power of data and story telling. You can influence decision through your stories but it has to be backed by data and presented in a comprehensible format and your one little presentation can really changes lives. I can go on giving a number of examples to show how it has motivated me, but I will stop here, and keep my Focus.\nSo, my encounter of Data Science is through excel (you can do a lot of stuff there too) and for spatial one if through GIS (Geographic Information System). I know some basics of SQL, R, Python, but not enough to score good in quiz. But I am betting on my time here and the aim is to have the most productive time this summer 2024 and see the result after 10 weeks. I will come back to share the updates!\n\n\nMy Leisure\nI do not really have any particular talents apart from the academics one, but apart from sleep and watching tv, I love a good conversation of getting to know new people, good food, some cooking, travelling, a little bit of gardening and reading fictions. Pretty ordinary things!"
  },
  {
    "objectID": "studentBlogs/tabassum/student-introductions/Tabassum_Introduction.html#never-stop-learning",
    "href": "studentBlogs/tabassum/student-introductions/Tabassum_Introduction.html#never-stop-learning",
    "title": "Tabassum’s Introduction",
    "section": "“NEVER STOP LEARNING”",
    "text": "“NEVER STOP LEARNING”"
  },
  {
    "objectID": "studentBlogs/solomon/student-introductions/Solomon_Introduction.html",
    "href": "studentBlogs/solomon/student-introductions/Solomon_Introduction.html",
    "title": "Solomon’s Introduction",
    "section": "",
    "text": "Hi! My name is Solomon Eshun. You can call me Solomon (he/him/his). I am from Ghana, West Africa, and I’m pursuing a PhD in Applied Mathematics at Iowa State University (as of Spring 2024).\n\nResearch Interests\n\nCausal Inference\nInfectious Disease\nData Integration\n\n\n\nEducation\n\nPhD Applied Mathematics, Iowa State University (08/2023 - 05/2024)\nMSc Applied Stats. & Data Science, University of Texas Rio Grande Valley (UTRGV) (08/2021 - 05/2023)\nBSc Mathematics, University of Mines and Technology (UMaT), Ghana (09/2016 - 08/2020)\n\n\n\nExperience\n\nGraduate Fellow, Data Science for the Public Good, Iowa State (05/2024 - Present)\nGraduate Teaching Assistant, Department of Mathematics, Iowa State (08/2023 - 05/2023)\nSustainability Research Fellow, Office for Sustainability, UTRGV (08/2022 - 05/2023)\nGraduate Teaching Assistant, School of Math. & Stats. Sciences, UTRGV (08/2021 - 05/2023)\nComputational Science Intern, Los Alamos National Lab, Los Alamos NM (05 - 08/2022)\nTeaching Assistant, Department of Mathematical Sciences, UMaT (09/2020 - 08/2021)\nData Production Intern, Ghana Statistical Service, Accra (05 - 08/2019)\n\n\n\nSomething interesting about myself, and what I hope to learn\nI take pride in my strong analytical and computational skills, tenacity, and dedication to continuous improvement. As I embark on the DSPG program, I seek to improve my leadership and mentoring capabilities, alongside acquiring new technical expertise. An important goal for me is to refine my communication skills to effectively convey data-driven insights to non-technical stakeholders. Additionally, I aim to master the management and extraction of value from extensive real-world datasets, leveraging these skills to make impactful decisions and contributions.\nIn my leisure time, I enjoy listening to music, watching soccer games and playing video games.\n\n\nWhy I Care about Data Science\nI remember the moment I became interested in advanced research in Data Science. It was during my master’s research, where I evaluated the effect of sodium-glucose co-transporter-2 (SGLT2) inhibitors on chronic kidney disease (CKD). I used propensity score matching to address differences in patient characteristics that might influence SGLT2 prescription. Additionally, I utilized machine learning models to predict CKD cases. Since the dataset was imbalanced, I applied balanced bagging with bootstrapping and synthetic minority oversampling to balance the distribution of CKD and non-CKD cases in the training data. This aimed to facilitate improved learning on the training dataset and enhance predictions on the testing data. The analysis revealed that individuals taking SGLT2 had fewer CKD cases than those who did not, highlighting the protective effect of SGLT2 on CKD. This shows the potential of statistics and data science in extracting meaningful insights from healthcare data.\nHowever, amidst this revelation, a significant issue surfaced - the absence of reliable data for marginalized populations, resulting in disproportionately worse health outcomes. Since then, my focus evolved from merely using models to make predictions to understanding the “why” and “what-if” behind those predictions. I realized that relying solely on supervised models was insufficient for this purpose, prompting me to develop methods that could integrate machine learning and causal approaches to address and explain predictions in imbalanced datasets.\n\n\n\nFigure Reference (Causality for Machine Learning)\n\n\nAs a consequence, I aim to leverage machine learning with causal methods to understand the influence of policies and interventions on diseases outcomes across various data sources. I seek to move from prediction-based models to learning independent causal models in areas like transfer and meta-learning. This approach will aid in integrating diverse healthcare datasets, often compounded by heterogeneous patient characteristics and medical protocols, ensuring the generalizability of my findings against external datasets. Additionally, I aim to create models capable of scaling up datasets to better reflect the diversity of the population. My goal is to enhance model interpretability through counterfactual reasoning and mitigate algorithmic biases for fair and equitable outcomes, ensuring the reliability of healthcare research across different demographics."
  },
  {
    "objectID": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#majors",
    "href": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#majors",
    "title": "Nhat’s Introduction",
    "section": "Majors",
    "text": "Majors\nA first-year student in the Iowa State honors program, struggling with a triple major in computer science, data science and applied mathematics. :("
  },
  {
    "objectID": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#data-science-with-me",
    "href": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#data-science-with-me",
    "title": "Nhat’s Introduction",
    "section": "Data Science with Me",
    "text": "Data Science with Me\nWhy data science? Imagine a world where every second brings new data and endless opportunities to uncover hidden patterns. Sounds like a interesting version of a treasure hunt, right? That means, becoming a data scientist guarantees that I’ll never run out of puzzles to solve, and let’s face it, puzzles are way more fun than Sudoku."
  },
  {
    "objectID": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#prior-experience",
    "href": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#prior-experience",
    "title": "Nhat’s Introduction",
    "section": "Prior Experience?",
    "text": "Prior Experience?\nMy passion for mathematics and logical challenges naturally drew me to the fascinating world of AI and data science. I don’t have much coding experience regarding these two areas, but I strongly believe my statistical and mathematical foundation is a great plus during my journey."
  },
  {
    "objectID": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#hobbies",
    "href": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#hobbies",
    "title": "Nhat’s Introduction",
    "section": "Hobbies",
    "text": "Hobbies\nOutside of my academic endeavors, I enjoy participating in mathematics competitions (because who doesn’t love a good brain workout?), developing software projects, and staying up-to-date with the latest tech trends. I also have a passion for leadership and community involvement, which has seen me take on various extracurricular activities.\n\n\n\n\nGertrude Herr Adamson Award ($1,000)\n\n\n\n\n\n\n\n\nIowa Collegiate Math Competition (1st Place)"
  },
  {
    "objectID": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#academic-and-professional-goals",
    "href": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#academic-and-professional-goals",
    "title": "Nhat’s Introduction",
    "section": "Academic and Professional Goals",
    "text": "Academic and Professional Goals\nI’m on a quest to climb the academic ladder, aiming for a Ph.D. degree after graduating. My current mission is to absorb as many advanced courses as possible and utilize online platforms like Coursera to build a robust foundation in machine learning. Also, I want to join a research, where I can leverage my skills to prepare for an intensive study in machine learning."
  },
  {
    "objectID": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#skills-and-development",
    "href": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#skills-and-development",
    "title": "Nhat’s Introduction",
    "section": "Skills and Development",
    "text": "Skills and Development\n\nValued Skills\n\nAnalytical thinking and problem-solving (a.k.a. being a data detective)\nProficiency in programming languages like Python\nStrong foundation in mathematical concepts and their applications (because math is the language of the universe, and who doesn’t want to speak universe?)\n\n\n\nSkills to Develop\n\nData exploration and visualization (turning data into beautiful, insightful art)\nData cleaning (because even data needs a good scrub sometimes)\nData processing (preparing data for the ultimate transformation)\nBasics of AI and machine learning knowledge (unlocking the secrets of smart machines) \n\n\n\nPursuing These Skills\nThese skills are essential to be well-prepared for future internships and professional jobs. Mastering them will allow me to tackle complex problems, effectively communicate insights, and stay competitive in the rapidly evolving field of data science."
  },
  {
    "objectID": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#interesting-fact",
    "href": "studentBlogs/nhat/student-introductions/Nhat_Introduction.html#interesting-fact",
    "title": "Nhat’s Introduction",
    "section": "Interesting Fact",
    "text": "Interesting Fact\nAn interesting fact about me is that I thrive on challenges and enjoy stepping out of my comfort zone. My background and experiences have shaped my academic journey and fueled my passion for machine learning and data science. I look forward to sharing my knowledge and learning from my peers at Iowa State.\nP.S. If you ever need a teammate for any development projects, I’m your person. Just bring some coffee."
  },
  {
    "objectID": "studentBlogs/neha.html",
    "href": "studentBlogs/neha.html",
    "title": "Neha Tirunagiri",
    "section": "",
    "text": "Neha’s Midpoint Reflection\n\n\n\n\n\n\nMidpoint Reflection\n\n\n\n\n\n\n\n\n\nJan 6, 2026\n\n\nNeha Tirunagiri\n\n\n\n\n\n\n\n\n\n\n\n\nNeha’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\nNeha Tirunagiri\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/naomi/student-introductions/NaomiMauss_Introduction.html",
    "href": "studentBlogs/naomi/student-introductions/NaomiMauss_Introduction.html",
    "title": "Naomi’s Introduction",
    "section": "",
    "text": "The basics\n\n\n\nI am originally from Tennessee, but moved to Iowa so I could attend Iowa State.\n\n\n\nI will enter my 3rd year at Iowa State this fall.\n\n\n\nI am currently majoring in data science and linguistics. I enjoy studying data science because I like learning about how I can take data and apply code in order to learn new things about how the world works. I am studying linguistics because I enjoy it, and hope someday to work on NLP."
  },
  {
    "objectID": "studentBlogs/manjul/student-introductions/Manjul_Introduction.html",
    "href": "studentBlogs/manjul/student-introductions/Manjul_Introduction.html",
    "title": "Manjul’s Introduction",
    "section": "",
    "text": "Hello! My name is Manjul Balayar, and I am an incoming senior majoring in Software Engineering with a minor in Data Science. An interesting fact about me is that I was born in Nepal and moved to Ames about ten years ago."
  },
  {
    "objectID": "studentBlogs/manjul/student-introductions/Manjul_Introduction.html#about-me",
    "href": "studentBlogs/manjul/student-introductions/Manjul_Introduction.html#about-me",
    "title": "Manjul’s Introduction",
    "section": "",
    "text": "Hello! My name is Manjul Balayar, and I am an incoming senior majoring in Software Engineering with a minor in Data Science. An interesting fact about me is that I was born in Nepal and moved to Ames about ten years ago."
  },
  {
    "objectID": "studentBlogs/manjul/student-introductions/Manjul_Introduction.html#hobbies",
    "href": "studentBlogs/manjul/student-introductions/Manjul_Introduction.html#hobbies",
    "title": "Manjul’s Introduction",
    "section": "Hobbies",
    "text": "Hobbies\n\n  Hackathons    Boxing    Trying New Cuisines \n\n\nWhy Software?\n\nHigh Demand and Security\n\nInitially, I was solely a Software Engineering major, but I declared my minor in Data Science last fall. My interest in AI began about a year ago, inspired by ChatGPT and a video by Google’s machine learning research team about a model for detecting cancerous cells.\n\n\n\n\nFuture Plans\nI aspire to work in the field of AI/ML. Many people don’t realize that AI and Data Science are closely related, with AI often considered a subset of Data Science. Before diving into models and algorithms, it’s crucial to understand data preprocessing, data visualization, feature engineering, statistics, etc. These skills are essential for building better models.\n\n\nMy Skills and Objectives\n\nPersonal Strengths\n\nGood communicator: An essential skill in any sector.\nAdaptability: I am a quick learner.\nOpen-mindedness: Being open-minded helps me adapt, learn, and grow in my field.\n\n\n\nData Science Skills\n\nExperience with data preprocessing\nFundamental knowledge of machine learning and deep learning\nProficiency in multiple programming languages, frameworks, and tools\n\n\n\nGoals at DSPG\n\nData Visualization: Mastering tools like seaborn, matplotlib, and Tableau, and understanding the significance of various graphs.\n\n\n\nMachine Learning Tools: Improving my proficiency with TensorFlow, Keras, PyTorch, Sci-kit Learn, and more.\n\n\n\nFeature Engineering: Learning to transform raw data into meaningful features.\n\n\nThese skills are crucial for my goal of working in AI and ML. Understanding and working with data is key to developing better models. I aim to improve through consistent practice and guidance from our graduate fellows and advisors."
  },
  {
    "objectID": "studentBlogs/lily.html",
    "href": "studentBlogs/lily.html",
    "title": "Lily Christenson",
    "section": "",
    "text": "Lily’s Midpoint Reflection\n\n\n\n\n\n\nMidpoint Reflection\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\nLily Christenson\n\n\n\n\n\n\n\n\n\n\n\n\nLily’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nLily Christenson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/karthik.html",
    "href": "studentBlogs/karthik.html",
    "title": "Karthik Hanumanthaiah",
    "section": "",
    "text": "Karthik’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\nKarthik Hanumanthaiah\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/joseph/student-introductions/Betti_Introduction.html#personal-background",
    "href": "studentBlogs/joseph/student-introductions/Betti_Introduction.html#personal-background",
    "title": "Joseph’s Introduction",
    "section": "Personal Background",
    "text": "Personal Background\n\nI am from Owatonna, Minnesota\nI am majoring in landscape architecture\nI am a senior\nI enjoy going on hikes\nI play guitar and bass\nI enjoy making art\nI make music and movies\n\n\nThe image to the right is a movie poster I made my Freshman year."
  },
  {
    "objectID": "studentBlogs/joseph/student-introductions/Betti_Introduction.html#goals-and-objectives",
    "href": "studentBlogs/joseph/student-introductions/Betti_Introduction.html#goals-and-objectives",
    "title": "Joseph’s Introduction",
    "section": "Goals and Objectives",
    "text": "Goals and Objectives\nAs a designer, it is important to understand the effects that our designs have on society and the environment. An important tool for analyzing the effects we have on the world is data science. As a young scholar at DSPG, I aim to improve my GIS skills and gain new analytic tools. I don’t have much experience with data science, but I am excited to learn more.\n\nObjectives for the Summer\n\nImprove at GIS\nLearn more about statistics\nLearn how to program (R, Python, SQL, etc.)\nLearn how to apply data science as a Landscape Architect\nGain real-world experience\n\n\n\n\n\nCareer Objectives\n\nWork as a landscape architect\nImprove the sustainability of landscapes\nProduce Equitable and accessible designs\nDesign with the community in mind\nImprove my understanding of social issues\nLearn new perspectives about art and design\n\n\nThe image to the right is a set of maps by Ian McHarg, one of the first landscape architects to use data in decision-making."
  },
  {
    "objectID": "studentBlogs/joseph/student-introductions/Betti_Introduction.html#strengths-and-areas-of-improvement",
    "href": "studentBlogs/joseph/student-introductions/Betti_Introduction.html#strengths-and-areas-of-improvement",
    "title": "Joseph’s Introduction",
    "section": "Strengths and Areas of Improvement",
    "text": "Strengths and Areas of Improvement\n\nStrengths\n\nCreativity\nCuriosity\nCompetence in ArcGIS Pro\nFast learner\nCritical Thinking Skills\nDesign-oriented\n\n\n\nAreas of Improvement\n\nProgramming\nGathering data\nAnalyzing data\nStatistics\nGIS\nand much, much more!"
  },
  {
    "objectID": "studentBlogs/harun/student-midpoint/harun-midpoint.html",
    "href": "studentBlogs/harun/student-midpoint/harun-midpoint.html",
    "title": "Harun’s Midpoint Reflection",
    "section": "",
    "text": "Coordinating the program up to this point has been a very rewarding experience. The students are making considerable progress and demonstrate their progress with every activity and presentation. I feel quite fortunate to have this cohort of young scholars and graduate fellows this year."
  },
  {
    "objectID": "studentBlogs/harun/student-midpoint/harun-midpoint.html#task-management",
    "href": "studentBlogs/harun/student-midpoint/harun-midpoint.html#task-management",
    "title": "Harun’s Midpoint Reflection",
    "section": "Task Management",
    "text": "Task Management\nMaking sure that things are logistically running through the program definitely required me to set up a system that I could rely on for effectively managing all of my tasks. I’ve been using an app called todoist to manage the various sorts of tasks that are required of me to complete throughout DSPG. These have included things such as:\n\nDate Reminders and Lists for Scheduling Guest Speakers\nRecurring Tasks for Updating Schedules, Blog Posts, and other Assignments\nOne-off Tasks Focused on Helping the Three Project Tasks\nOutlining Tasks Related to Program Finances and Administration.\n\nHaving a system for being able to categorize and set up dates, priorities, and labels for my tasks has been great for staying on top of the administrative tasks."
  },
  {
    "objectID": "studentBlogs/harun/student-midpoint/harun-midpoint.html#management-systems",
    "href": "studentBlogs/harun/student-midpoint/harun-midpoint.html#management-systems",
    "title": "Harun’s Midpoint Reflection",
    "section": "Management Systems",
    "text": "Management Systems\nWhile a systematic list of all the tasks has been helpful, executing these also required me to come up with repeatable processes.\n\nBlog System (Quarto)\nHaving everything consolidated into one place helped to give students a single landing page for work related to the program. In the previous years, the schedule would be a word document on the CyBox, the schedule for talks was tabulated in an excel sheet, and other pieces of information scattered through various systems and directories. Only needing to update a single location for informing students and project advisors who are not always with the group is both efficient and saves a lot of time.\n\n\nGitHub Repositories, Templates, and Classrooms\nIn the interest of having students work on assignments with a mix of datasets, code, and documentation, I’ve constructed all assignments to be GitHub Templates that can be reused for assignment creation in GitHub Classrooms. This has made collecting the work of students significantly easier and has the added bonus of being available for future DSPG programs since they are template repositories. People talk about reproducable code, but reproducible assignments are really where its at!"
  },
  {
    "objectID": "studentBlogs/harun/student-midpoint/harun-midpoint.html#coding-technological",
    "href": "studentBlogs/harun/student-midpoint/harun-midpoint.html#coding-technological",
    "title": "Harun’s Midpoint Reflection",
    "section": "Coding & Technological",
    "text": "Coding & Technological\nWith one of the projects focusing on Machine Learning and AI through the use of High Performance Computing (HPC), I’ve also gotten the opportunity to work with the HPC systems to get an understanding of how they work and how to execute tasks on them. The HPC has forced me to be more friendly with the Command Line Interface which I’ve been using more as a way to understand program execution on computers.\nI’ve also gotten some more experience working with loading up and operationalizing LLMs, but I hope to learn more as the program continues and as the Housing & AI team makes more progress."
  },
  {
    "objectID": "studentBlogs/harun/student-midpoint/harun-midpoint.html#github-actions",
    "href": "studentBlogs/harun/student-midpoint/harun-midpoint.html#github-actions",
    "title": "Harun’s Midpoint Reflection",
    "section": "GitHub Actions",
    "text": "GitHub Actions\nAlong with gaining more experience on the CLI, I’ve also been working with GitHub Actions as a way to automate the tasks that GitHub Classrooms and GitHub Repository Templates have not been able to help with.\n\n\n\nGitHub Actions Automated Work-Flow\n\n\nThis process removes a lot of the tedious work that comes with having to sync repositories and render them only to have a post updated for a typo and therefore forcing the whole cycle all over again (trust me, this is lived experience talking).\nThis gets very time consuming even for twelve students."
  },
  {
    "objectID": "studentBlogs/harun/student-midpoint/harun-midpoint.html#program-development",
    "href": "studentBlogs/harun/student-midpoint/harun-midpoint.html#program-development",
    "title": "Harun’s Midpoint Reflection",
    "section": "Program Development",
    "text": "Program Development\nI won’t go into the specifics of them all here but I’m trying my best to document my rationale behind implementing some of the newer components of DSPG, like the student group activities. The ultimate goal is to have this all in some kind of handbook which serves to better situate the purpose and design of the DSPG program. Definitely a Work-In-Progress now, but hoping that as I get more free time, I am able to contribute further to it."
  },
  {
    "objectID": "studentBlogs/harun/student-midpoint/harun-midpoint.html#things-time-wont-allow-me",
    "href": "studentBlogs/harun/student-midpoint/harun-midpoint.html#things-time-wont-allow-me",
    "title": "Harun’s Midpoint Reflection",
    "section": "Things Time Won’t Allow Me",
    "text": "Things Time Won’t Allow Me\nIn my introductory blog post, I meantioned that I was also interested in learning more about statistical modelling. Unfortunately, I’ve not made any significant progress in this area due to how much the logistical components of DSPG consume my time.\nI look forward to keeping it in my list for future learning."
  },
  {
    "objectID": "studentBlogs/harun.html",
    "href": "studentBlogs/harun.html",
    "title": "Harun Celik",
    "section": "",
    "text": "Harun’s Midpoint Reflection\n\n\n\n\n\n\n\n\n\n\n\nJun 24, 2024\n\n\nHarun Celik\n\n\n\n\n\n\n\n\n\n\n\n\nHarun’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nHarun Celik\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/cael.html",
    "href": "studentBlogs/cael.html",
    "title": "Cael Leistikow",
    "section": "",
    "text": "Cael’s Midpoint Blog\n\n\n\n\n\n\nMidpoint Reflection\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\nCael Leistikow\n\n\n\n\n\n\n\n\n\n\n\n\nCael’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nCael Leistikow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/alister/student-introductions/Alister_Introduction.html",
    "href": "studentBlogs/alister/student-introductions/Alister_Introduction.html",
    "title": "Alister’s Introduction",
    "section": "",
    "text": "Hi, my name is Alister! I originated from Malaysia and I started my journey in Iowa State University during the fall 2023. I’m a sophomore majoring in Data Science."
  },
  {
    "objectID": "studentBlogs/alister/student-introductions/Alister_Introduction.html#personal-background",
    "href": "studentBlogs/alister/student-introductions/Alister_Introduction.html#personal-background",
    "title": "Alister’s Introduction",
    "section": "",
    "text": "Hi, my name is Alister! I originated from Malaysia and I started my journey in Iowa State University during the fall 2023. I’m a sophomore majoring in Data Science."
  },
  {
    "objectID": "studentBlogs/alister/student-introductions/Alister_Introduction.html#interesting-fact-about-me",
    "href": "studentBlogs/alister/student-introductions/Alister_Introduction.html#interesting-fact-about-me",
    "title": "Alister’s Introduction",
    "section": "Interesting fact about me!",
    "text": "Interesting fact about me!\n\nI am an outdoor enthusiast who spent most of my free time playing sports and hanging out with friends. Occasionally, you will also find me spending hours reading books in local cafes, enjoying a break from the busyness of student life. I would also consider myself a huge movie person and I still recalled my roomates and I spent 10 hours binge watching, “Haunting of the Hill House” during the spring break."
  },
  {
    "objectID": "studentBlogs/alister/student-introductions/Alister_Introduction.html#why-data-science",
    "href": "studentBlogs/alister/student-introductions/Alister_Introduction.html#why-data-science",
    "title": "Alister’s Introduction",
    "section": "Why Data Science?",
    "text": "Why Data Science?\n\n\nMy interest in Data Science stems from my deep passion to explore and figuring out the underlying meaning behind the data and the vast impact from data findings fascinate me. As people always say, “data is the new future”, pursuing a major in data science allows me to use my skillset in helping the communities to make better decisions through data. After graduation, I plan to pursue a career as a data scientist in the business and finance field. I am drawn to the intricate patterns and strategic decisions that data can unveil in financial markets and business operations. The ability to analyze complex datasets, uncover trends, and make data-driven decisions excites and motivates me. Also, I hope to be able to help the customers to make better financial decisions, identify frauds, reduce risks based on data-driven insights."
  },
  {
    "objectID": "studentBlogs/alister/student-introductions/Alister_Introduction.html#goals-and-objectives",
    "href": "studentBlogs/alister/student-introductions/Alister_Introduction.html#goals-and-objectives",
    "title": "Alister’s Introduction",
    "section": "Goals and Objectives",
    "text": "Goals and Objectives\n\nMy experience with Data Science\n\nI have taken several introductory Data Science courses thus far and I have experience performing data analysis and data visualization on a customers purchasing behavior dataset taken from Kaggle. I am familiar with Pandas and Matplotlib using Python to perform data preprocessing, cleaning, analysis and visualization. Working with data in a team setting for the first time was an eye-opening experience as I get to collaborate with students from various background and getting insights and learning from one another. To make full use of my summer, I plan to enroll in several online courses for R and AI to buck up my skills.\n\n\n\nThree skills that I would like to learn from the DSPG program\n\nI would love to gain a deeper understanding on AI and its application on a real-world datasets. I would like to use this program as a stepping stone to allow me to apply AI under the guidance of mentors. Besides AI, I would like to use this opportunity to further enhance my programming skills specifically in Python and R. Lastly, beyond the technical skills, I hope to improve my ability to communicate and explain complex technical data findings to non-technical stakeholders.\n\n\n\nWhy the three skills\n\n\nApplying AI to real-world datasets allows me to understand practical challenges and develop solutions that are impactful and relevant to industry needs more efficiently\nHaving strong programming skills enable me to implement data analysis and visualization effectively as Python and R are one of the primary languages used in data science\nWhile dealing with sophiscated AI models and analyses, having the ability to communicate my findings clearly to stakeholders ensure that my work can be fully utilized and impactful towards the intended audiences\n\n\n\n\nSteps that I would take to improve my skills\n\n\nHave an open mind by experimenting with different data science tools and to seek mentorship from peers\nPratice working on real-world datasets to gain practical experience (participate in DSPG, hackathons)\nPractice presenting my findings through reports, presentations, and visualizations as well as constant engage activity such as public speaking"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Weekly Schedule",
    "section": "",
    "text": "Jump to This Week’s Schedule"
  },
  {
    "objectID": "schedule.html#may-15th-wednesday",
    "href": "schedule.html#may-15th-wednesday",
    "title": "Weekly Schedule",
    "section": "May 15th – Wednesday",
    "text": "May 15th – Wednesday\n\nLocation\n\nDurham 171\n\nDaily Goals\n\nDSPG Introductions, Data Camp Assessments, and Data Team Activity\n\n\n\n8:00 – 8:30:\n\nIntroduction to the DSPG Program (Slide Deck of Past Projects and General Goals)\nIntroductions between DSPG Scholars, Fellows, and Project Advisers\n\n8:30 – 9:30:\n\nShare information about student expectations\nShare scheduling information\nShare calendar information of the program\n\n9:30 - 12:00:\n\nDataCamp Assessments\n\nAnalytic Fundamentals\nR Programming\nPython Programming\nAI Fundamentals\n\n\n12:00 - 1:00: Lunch Break\n1:00 - 1:15: Data Team Activity Introduction\n1:15 - 3:15: Data Team Activity Work Time\n3:15 - 4:00: Data Team Activity Presentations"
  },
  {
    "objectID": "schedule.html#may-16th-thursday",
    "href": "schedule.html#may-16th-thursday",
    "title": "Weekly Schedule",
    "section": "May 16th – Thursday",
    "text": "May 16th – Thursday\n\nLocation\n\nDurham 171\n\nDaily Goals\n\nGit, GitHub, Teams, Outlook Calendar, CyBox, and Blogs Setup\n\n\n\n8:00 – 10:00:\n\nCreate a GitHub Account and Join GitHub Education\nRead through What is Git? The Complete Guide to Git\nComplete the Introduction to GitHub Lesson\nComplete the Review Pull Requests Lesson\nComplete the Resolve Merge Conflicts Lesson\nComplete the Communicate Using Markdown Lesson\nOther GitHub Skills to Explore\n\n10:00 – 10:30:\n\nGo over the Teams Channels\nGo over CyBox setups\nGo over Outlook Calendar\n\n10:30 – 12:00:\n\nIntroduction to the Blogging System\nIntroduction to GitHub Classrooms System\nQuarto Blogs Setup and Training\n\n12:00 – 1:00: Lunch Break\n1:00 – 4:00:\n\nStudents Receive First Blog Assignment on Their Introductions\nRender Blog with Student Files at the End of Day"
  },
  {
    "objectID": "schedule.html#may-17th-friday",
    "href": "schedule.html#may-17th-friday",
    "title": "Weekly Schedule",
    "section": "May 17th – Friday",
    "text": "May 17th – Friday\n\nLocation\n\nCarver 0268 (EVERY FRIDAY)\n\nDaily Goals\n\nBlog Student Introduction Presentations and DataCamp\n\n\n\n8:00 – 12:00:\n\nStudent Introduction Presentation Preparations\nDataCamp Training\n\nSelect either of the Introductory R or Python Track Assignments\n\n\n12:00 - 1:00: Lunch Break\n1:00 – 3:00: Student Introduction Presentations"
  },
  {
    "objectID": "schedule.html#may-20th-monday",
    "href": "schedule.html#may-20th-monday",
    "title": "Weekly Schedule",
    "section": "May 20th – Monday",
    "text": "May 20th – Monday\n\nLocation\n\nEast Hall 311 (EVERY MON-THUR)\n\nDaily Goals\n\nTidycensus Training\n\n\n\n8:00 – 9:00: Graphics Monday Exercise\n9:00 - 12:00: Tidycensus Training Starts\n\nIntroduction: Read through the Topics below on Census Reporter\n\nGetting Started\nAbout the Census\nGeography\nLook through the Table Codes to get an idea of what’s available\nThe Census Subjects headings detail how each variable is collected and what tables are associated with those variables\n\nPractice in R: Read through the Designated Sections of the Analyzing US Census Data Book\n\nReview the material from the Preface up to Chapter 5: Census Geographic Data and Applications in R\n\n\n12:00 - 1:00: Lunch Break\n1:00 - 4:00: Tidycensus Training Continues\n\nRecent Materials: Review the Working with the 2022 ACS and Analyzing 2020 Decennial US Census Data in R presentations. There is a lot of overlap with the book, but you can click the hamburger icon on the bottom left to navigate the menu when its information you’ve already learned.\nCensus Mapping: Go through the Doing ‘GIS’ and making maps with US Census data in R presentation. This will be new information so practice the code on the presentation with areas of interest to you. You can use these materials for your upcoming blog post."
  },
  {
    "objectID": "schedule.html#may-21st-tuesday",
    "href": "schedule.html#may-21st-tuesday",
    "title": "Weekly Schedule",
    "section": "May 21st – Tuesday",
    "text": "May 21st – Tuesday\n\nLocation\n\nEast Hall 311\n\nDaily Goals\n\nProject Assignments, Matthew Voss on Quarto Documents, and Tidycensus assignment\n\n\n\n8:00 – 9:00: Team Assignments and Project Introductions\n9:00 - 12:00: TidyCensus Assignment Start\n12:00 - 1:00: Lunch Break\n1:00 - 2:00: Presentation on Quarto Document Generation\n2:00 - 4:00: Complete Tidycensus Assignment"
  },
  {
    "objectID": "schedule.html#may-22nd-wednesday",
    "href": "schedule.html#may-22nd-wednesday",
    "title": "Weekly Schedule",
    "section": "May 22nd – Wednesday",
    "text": "May 22nd – Wednesday\n\nLocation\n\nEast Hall 311\n\nDaily Goals\n\nStudents Hold Their First Client Meeting\n\n\n\n8:00 - 9:00: Complete\n9:00 - 10:00: Start Project Specific DataCamp Training\n1:00 - 4:00: Client Meetings for All Teams\n\nZoning Guide Meets at 2321 N Loop Dr. Suite 121 (link to Google Maps)\nHousing and AI Meets at 2321 N Loop Dr. Second Floor West Conference Room\nCapitals at East Hall 311"
  },
  {
    "objectID": "schedule.html#may-23rd-thursday",
    "href": "schedule.html#may-23rd-thursday",
    "title": "Weekly Schedule",
    "section": "May 23rd – Thursday",
    "text": "May 23rd – Thursday\n\nLocation\n\nEast Hall 311\n\n\n*Zoning Guide Team Completes Their Assignments from Client Meeting\n\n8:00 - 9:00: Solomon Eshun Presentation\n9:00 - 12:00: Project Specific Training Continues\n12:00 - 1:00: Lunch Break\n1:00 - 2:00: Guest Speaker - Félix A. Báez-Santiago\n2:00 - 4:00: Project Specific Training Continues"
  },
  {
    "objectID": "schedule.html#may-24th---friday",
    "href": "schedule.html#may-24th---friday",
    "title": "Weekly Schedule",
    "section": "May 24th - Friday",
    "text": "May 24th - Friday\n\nLocation\n\nCarver 0268\n\n\nDSPG is Half-day - Work Until 1:00\n\n8:00 - 10:00: Complete Tidycensus-Retail Trade Presentations\n10:00 - 12:00: Tidycensus-Retail Trade Presentations\n12:00 - 1:00: DataCamp Training\n\nRest of the Day is Off"
  },
  {
    "objectID": "schedule.html#may-27th---monday",
    "href": "schedule.html#may-27th---monday",
    "title": "Weekly Schedule",
    "section": "May 27th - Monday",
    "text": "May 27th - Monday\nMemorial Day Break - No DSPG"
  },
  {
    "objectID": "schedule.html#may-28th---tuesday",
    "href": "schedule.html#may-28th---tuesday",
    "title": "Weekly Schedule",
    "section": "May 28th - Tuesday",
    "text": "May 28th - Tuesday\n\nLocation\n\nEast Hall 311\n\nDaily Goals\n\nStart Project Specific Work\n\n\n\n8:00 - 9:00: Data Science Trivia\n9:00 - 12:00: Project Members Meet with their Teams\n12:00 - 1:00: Lunch Break\n1:00 - 4:00: Project Specific Work Continues"
  },
  {
    "objectID": "schedule.html#may-29th---wednesday",
    "href": "schedule.html#may-29th---wednesday",
    "title": "Weekly Schedule",
    "section": "May 29th - Wednesday",
    "text": "May 29th - Wednesday\n\nLocation\n\nEast Hall 311\n\nDaily Goals\n\nContinue Project Work & Census Training with Sandy\n\n\n\n8:00 - 9:00: Housing Data Sources Inventory\n9:00 - 10:00: Project Specific Work Continues\n10:00 - 12:00: Sandra Burke Census Presentation\n12:00 - 1:00: Lunch Break\n1:00 - 4:00: Project Specific Work Continues"
  },
  {
    "objectID": "schedule.html#may-30th---thursday",
    "href": "schedule.html#may-30th---thursday",
    "title": "Weekly Schedule",
    "section": "May 30th - Thursday",
    "text": "May 30th - Thursday\n\nLocation\n\nEast Hall 311\n\nDaily Goals\n\nContinue Project Work & Learn about Coding Functions with Karthik Hanumanthaiah\n\n\n\nAll Day: Project Specific Work Continues"
  },
  {
    "objectID": "schedule.html#may-31st---friday",
    "href": "schedule.html#may-31st---friday",
    "title": "Weekly Schedule",
    "section": "May 31st - Friday",
    "text": "May 31st - Friday\n\nLocation\n\nCarver 0268\n\nDaily Goals\n\nCreate First Weekly Team Blog\n\n\n\n8:00 - 10:00: Complete this Week’s Project Goals\n10:00 - 12:00: Student’s Create Weekly Team Blog\n12:00 - 1:00: Lunch Break\n1:00 - 3:00: Student’s Present Team Weekly Blog\n3:00 - 4:00: Review Feedback and Submit Final weekly Blog"
  },
  {
    "objectID": "schedule.html#june-3rd---monday",
    "href": "schedule.html#june-3rd---monday",
    "title": "Weekly Schedule",
    "section": "June 3rd - Monday",
    "text": "June 3rd - Monday\n\nLocation\n\nEast Hall 311\n\nDaily Goals\n\nContinue Project Work & Functions Presentation\n\n\n\n8:00 - 10:00: Karthik Hanumanthaiah Functions Presentation\nRest of Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-4th---tuesday",
    "href": "schedule.html#june-4th---tuesday",
    "title": "Weekly Schedule",
    "section": "June 4th - Tuesday",
    "text": "June 4th - Tuesday\n\nLocation\n\nEast Hall 311\n\nDaily Goals\n\nGuest Speaker\n\n\n\n8:00 - 11:30: Continue Project Work\n11:30 - 12:30: Guest Speaker - Dr. Cass Dorius\n12:30 - 1:30: Lunch Break\n1:30 - 4:00: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-5th---wednesday",
    "href": "schedule.html#june-5th---wednesday",
    "title": "Weekly Schedule",
    "section": "June 5th - Wednesday",
    "text": "June 5th - Wednesday\n\nLocation\n\nEast Hall 311\n\n\n\n8:00 - 10:00: Graphics Activity\nRest of Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-6th---thursday",
    "href": "schedule.html#june-6th---thursday",
    "title": "Weekly Schedule",
    "section": "June 6th - Thursday",
    "text": "June 6th - Thursday\n\nLocation\n\nEast Hall 311\n\n\n\n8:00 - 9:00: Assign Personal Blog Assignment\nRest of Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-7th---friday",
    "href": "schedule.html#june-7th---friday",
    "title": "Weekly Schedule",
    "section": "June 7th - Friday",
    "text": "June 7th - Friday\n\nLocation\n\nCarver 0268\n\n\n\n8:00 - 9:00: Data Science Trivia\n9:00 - 12:00: Complete Project Work\n12:00 - 1:00: Lunch Break\n1:00 - 2:30: Complete Weekly Team Blogs\n2:30 - 4:00: Present Team Blogs"
  },
  {
    "objectID": "schedule.html#june-10th---monday",
    "href": "schedule.html#june-10th---monday",
    "title": "Weekly Schedule",
    "section": "June 10th - Monday",
    "text": "June 10th - Monday\n\nLocation\n\nEast Hall 311\n\n\n\n8:00 - 9:00: Last Week’s Blog Wrap-Up & Presentations\n9:00 - 11:00: Continue Project Work\n11:00 - 12:00: Jay Maxwell - ArcGIS Pro Workshop\nRest of Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-11th---tuesday",
    "href": "schedule.html#june-11th---tuesday",
    "title": "Weekly Schedule",
    "section": "June 11th - Tuesday",
    "text": "June 11th - Tuesday\n\nLocation\n\nEast Hall 311\n\n\n\nAll Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-12th---wednesday",
    "href": "schedule.html#june-12th---wednesday",
    "title": "Weekly Schedule",
    "section": "June 12th - Wednesday",
    "text": "June 12th - Wednesday\n\nLocation\n\nEast Hall 311\n\n\n\n8:00 - 11:30: Continue Project Work\n11:30 - 12:30: Lunch Break\n1:00 - 2:00: Guest Speaker - Dr. Scott Samuelson\nRest of Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-13th---thursday",
    "href": "schedule.html#june-13th---thursday",
    "title": "Weekly Schedule",
    "section": "June 13th - Thursday",
    "text": "June 13th - Thursday\n\nLocation\n\nEast Hall 311\n\n\n\n8:00 - 9:30: Graphics Activity\nRest of Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-14th---friday",
    "href": "schedule.html#june-14th---friday",
    "title": "Weekly Schedule",
    "section": "June 14th - Friday",
    "text": "June 14th - Friday\n\nLocation\n\nCarver 0268\n\n\n\n8:00 - 9:00: Data Science Trivia\n9:00 - 12:00: Complete Project Work\n12:00 - 1:00: Lunch Break\n1:00 - 3:00: Complete Weekly Team Blogs\n3:00 - 4:00: Present Team Blogs"
  },
  {
    "objectID": "schedule.html#june-17th---monday",
    "href": "schedule.html#june-17th---monday",
    "title": "Weekly Schedule",
    "section": "June 17th - Monday",
    "text": "June 17th - Monday\n\nLocation\n\nEast Hall 311\n\n\n\n8:00 - 9:00: Last Week’s Team Blog & Personal Blog Completion\n9:00 - 12:00: ITAG Presentation Work\nRest of Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-18th---tuesday",
    "href": "schedule.html#june-18th---tuesday",
    "title": "Weekly Schedule",
    "section": "June 18th - Tuesday",
    "text": "June 18th - Tuesday\n\nLocation\n\nEast Hall 311\n\n\n\n8:00 - 9:45: Graphics Activity\n9:45 - 12:00: ITAG Presentation Work\nRest of Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-19th---wednesday",
    "href": "schedule.html#june-19th---wednesday",
    "title": "Weekly Schedule",
    "section": "June 19th - Wednesday",
    "text": "June 19th - Wednesday\n\nLocation\n\nEast Hall 311\n\n\n\n8:00 - 12:00: Continue Project Work\n12:00 - 1:00: Lunch Break\n2:00 - 3:00: ITAG Mock Presentations"
  },
  {
    "objectID": "schedule.html#june-20th---thursday",
    "href": "schedule.html#june-20th---thursday",
    "title": "Weekly Schedule",
    "section": "June 20th - Thursday",
    "text": "June 20th - Thursday\n\nLocation\n\nMeet Across Lake Laverne at 7:00AM\n\n\n\nAll Day: ITAG"
  },
  {
    "objectID": "schedule.html#june-21st---friday",
    "href": "schedule.html#june-21st---friday",
    "title": "Weekly Schedule",
    "section": "June 21st - Friday",
    "text": "June 21st - Friday\n\nLocation\n\nCarver 0268\n\n\n\n8:00 - 9:00: Data Science Trivia\n9:00 - 12:00: Complete Project Work\n12:00 - 1:00: Lunch Break\n1:00 - 3:00: Complete Weekly Team Blogs\n3:00 - 4:00: Present Team Blogs"
  },
  {
    "objectID": "schedule.html#june-24th---monday",
    "href": "schedule.html#june-24th---monday",
    "title": "Weekly Schedule",
    "section": "June 24th - Monday",
    "text": "June 24th - Monday\n\nLocation\n\nEast Hall 311\n\n\n\n8:00 - 9:00: Graphics Activity\nRest of Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-25th---tuesday",
    "href": "schedule.html#june-25th---tuesday",
    "title": "Weekly Schedule",
    "section": "June 25th - Tuesday",
    "text": "June 25th - Tuesday\n\nLocation\n\nEast Hall 311\n\n\n\nThroughout the Day: Student One-on-One Meetings\nAll Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-26th---wednesday",
    "href": "schedule.html#june-26th---wednesday",
    "title": "Weekly Schedule",
    "section": "June 26th - Wednesday",
    "text": "June 26th - Wednesday\n\nLocation\n\nEast Hall 311\n\n\n\nThroughout the Day: Student One-on-One Meetings\nThroughout the Day: Client Meetings"
  },
  {
    "objectID": "schedule.html#june-27th---thursday",
    "href": "schedule.html#june-27th---thursday",
    "title": "Weekly Schedule",
    "section": "June 27th - Thursday",
    "text": "June 27th - Thursday\n\nLocation\n\nEast Hall 311\n\n\n\n8:00 - 9:00: Data Science Trivia\nThroughout the Day: Student One-on-One Meetings\nRest of Day: Continue Project Work"
  },
  {
    "objectID": "schedule.html#june-28th---friday",
    "href": "schedule.html#june-28th---friday",
    "title": "Weekly Schedule",
    "section": "June 28th - Friday",
    "text": "June 28th - Friday\n\nLocation\n\nCarver 0268\n\n\n\n8:00 - 12:00: Continue Project Work\n1:00 - 3:00: Weekly Blog Preparations\n3:00 - 4:00: Weekly Blog Presentations"
  },
  {
    "objectID": "schedule.html#july-1st---monday",
    "href": "schedule.html#july-1st---monday",
    "title": "Weekly Schedule",
    "section": "July 1st - Monday",
    "text": "July 1st - Monday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-2nd---tuesday",
    "href": "schedule.html#july-2nd---tuesday",
    "title": "Weekly Schedule",
    "section": "July 2nd - Tuesday",
    "text": "July 2nd - Tuesday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-3rd---wednesday",
    "href": "schedule.html#july-3rd---wednesday",
    "title": "Weekly Schedule",
    "section": "July 3rd - Wednesday",
    "text": "July 3rd - Wednesday\n\nLocation\n\nEast Hall 311\n\n\nTentative: DSPG is Half-day - Work Until 1:00"
  },
  {
    "objectID": "schedule.html#july-4th---thursday",
    "href": "schedule.html#july-4th---thursday",
    "title": "Weekly Schedule",
    "section": "July 4th - Thursday",
    "text": "July 4th - Thursday\nJuly 4th Break - No DSPG"
  },
  {
    "objectID": "schedule.html#july-5th---friday",
    "href": "schedule.html#july-5th---friday",
    "title": "Weekly Schedule",
    "section": "July 5th - Friday",
    "text": "July 5th - Friday\nJuly 4th Break - No DSPG"
  },
  {
    "objectID": "schedule.html#july-8th---monday",
    "href": "schedule.html#july-8th---monday",
    "title": "Weekly Schedule",
    "section": "July 8th - Monday",
    "text": "July 8th - Monday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-9th---tuesday",
    "href": "schedule.html#july-9th---tuesday",
    "title": "Weekly Schedule",
    "section": "July 9th - Tuesday",
    "text": "July 9th - Tuesday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-10th---wednesday",
    "href": "schedule.html#july-10th---wednesday",
    "title": "Weekly Schedule",
    "section": "July 10th - Wednesday",
    "text": "July 10th - Wednesday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-11th---thursday",
    "href": "schedule.html#july-11th---thursday",
    "title": "Weekly Schedule",
    "section": "July 11th - Thursday",
    "text": "July 11th - Thursday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-12th---friday",
    "href": "schedule.html#july-12th---friday",
    "title": "Weekly Schedule",
    "section": "July 12th - Friday",
    "text": "July 12th - Friday\n\nLocation\n\nCarver 0268"
  },
  {
    "objectID": "schedule.html#july-15th---monday",
    "href": "schedule.html#july-15th---monday",
    "title": "Weekly Schedule",
    "section": "July 15th - Monday",
    "text": "July 15th - Monday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-16th---tuesday",
    "href": "schedule.html#july-16th---tuesday",
    "title": "Weekly Schedule",
    "section": "July 16th - Tuesday",
    "text": "July 16th - Tuesday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-17th---wednesday",
    "href": "schedule.html#july-17th---wednesday",
    "title": "Weekly Schedule",
    "section": "July 17th - Wednesday",
    "text": "July 17th - Wednesday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-18th---thursday",
    "href": "schedule.html#july-18th---thursday",
    "title": "Weekly Schedule",
    "section": "July 18th - Thursday",
    "text": "July 18th - Thursday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-19th---friday",
    "href": "schedule.html#july-19th---friday",
    "title": "Weekly Schedule",
    "section": "July 19th - Friday",
    "text": "July 19th - Friday\n\nLocation\n\nCarver 0268"
  },
  {
    "objectID": "schedule.html#july-22nd---monday",
    "href": "schedule.html#july-22nd---monday",
    "title": "Weekly Schedule",
    "section": "July 22nd - Monday",
    "text": "July 22nd - Monday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-23rd---tuesday",
    "href": "schedule.html#july-23rd---tuesday",
    "title": "Weekly Schedule",
    "section": "July 23rd - Tuesday",
    "text": "July 23rd - Tuesday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-24th---wednesday",
    "href": "schedule.html#july-24th---wednesday",
    "title": "Weekly Schedule",
    "section": "July 24th - Wednesday",
    "text": "July 24th - Wednesday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-25th---thursday",
    "href": "schedule.html#july-25th---thursday",
    "title": "Weekly Schedule",
    "section": "July 25th - Thursday",
    "text": "July 25th - Thursday\n\nLocation\n\nEast Hall 311"
  },
  {
    "objectID": "schedule.html#july-26th---friday",
    "href": "schedule.html#july-26th---friday",
    "title": "Weekly Schedule",
    "section": "July 26th - Friday",
    "text": "July 26th - Friday\n\nLocation\n\nCarver 0268"
  },
  {
    "objectID": "participants.html",
    "href": "participants.html",
    "title": "Program Participants",
    "section": "",
    "text": "Program Leader\n\nChristopher J. Seeger, Professor, Landscape Architecture; Extension Specialist in Geospatial Technologies\n\nProgram Coordinator\n\nHarun Çelik, History and Graduate GIS Certificate\n\nProject Advisers\n\nLisa Bates, Assistant Director, Community and Economic Development\nLiesl Eathington, Research Scientist III\nMatthew Voss, Rural Policy Data Analyst at Public Science Collaborative\nJay Maxwell, Data Analyst in Community and Economic Development Extension\n\nProgram Advisers\n\nAdisak Sukul, Ph.D. Associate Teaching Professor\nHeike Hoffman, Professor\nTodd Abraham, Research Scientist\n\nGraduate Fellows\n\nKarthik Hanumanthaiah, Computer Science\nTabassum Zaman, Community and Regional Planning\nSolomon Eshun, Applied Mathematics\n\nUndergraduate Interns\n\nAlister Gan, Data Science\nBlake Underwood, Data Science and Economics\nCael Leistikow, Landscape Architecture\nLily Christenson, Business and Statistics\nNaomi Mauss, Data Science\nNhat Le, Data Science, Math, and Computer Science\nManjul Balayar, Computer Science"
  },
  {
    "objectID": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-iowa-zoning-guide/Weekly_Team_Blog.html",
    "href": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-iowa-zoning-guide/Weekly_Team_Blog.html",
    "title": "Iowa Zoning Guide Week Two",
    "section": "",
    "text": "Looking Forward\n\n\nMoving forwards, we plan to continue to catalogue zoning data for 330 cities in Iowa. Additionally, we hope to scrape data from Redfin and download Iowa County Assessor Data so that we can take that information and use it to answer some of the questions to which we want to find answers.\n\n\nSources\n\n\nWe primarily worked with data we collected from the county zoning form we have been filling out, but some data was also taken from tidycensus. Redfin and county assessor data will be valuable resources in the future."
  },
  {
    "objectID": "allBlogs.html",
    "href": "allBlogs.html",
    "title": "Project Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 14, 2024\n\n\nIowa Zoning Guide Week Three\n\n\nIowa Zoning Guide\n\n\n\n\nJun 14, 2024\n\n\nHousing and AI Week Three\n\n\nHousing and AI\n\n\n\n\nJun 14, 2024\n\n\nCommunity Capitals Week Three\n\n\nCommunity Capitals\n\n\n\n\nJun 7, 2024\n\n\nIowa Zoning Guide Week Two\n\n\nIowa Zoning Guide\n\n\n\n\nJun 7, 2024\n\n\nHousing and AI Week Two\n\n\nHousing and AI\n\n\n\n\nJun 7, 2024\n\n\nCommunity Capitals Week Two\n\n\nCommunity Capitals\n\n\n\n\nMay 31, 2024\n\n\nIowa Zoning Guide Week One\n\n\nIowa Zoning Guide\n\n\n\n\nMay 31, 2024\n\n\nHousing and AI Week One\n\n\nHousing and AI\n\n\n\n\nMay 31, 2024\n\n\nCommunity Capitals Week One\n\n\nCommunity Capitals\n\n\n\n\nMay 6, 2024\n\n\nBlog Writing Guidelines\n\n\nHarun Celik\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#introduction",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#introduction",
    "title": "Iowa Zoning Guide Week One",
    "section": "Introduction",
    "text": "Introduction\nWe are building an Iowa zoning guide.\n\n\nNew Hampshire’s atlas demonstrates what were building.\n\n\nWe are collecting the data by looking at each city’s zoning ordinance, which provides a detailed breakdown of the zoning laws for the city.\n\n\nWe are mapping the atlas in ArcGIS.\n\n\nOur process is not the same as the one used by New Hampshire, but the outcome will be similar."
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#explaining-districts",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#explaining-districts",
    "title": "Iowa Zoning Guide Week One",
    "section": "Explaining districts",
    "text": "Explaining districts\nWhat is the point of a housing district?\n\n\nIt’s to maintain public safety; ideally, people won’t live right next to a chemical plant. Light quality, air quality, and access to green space are all affected by zoning regulations.\n\n\nZoning code affects these by mandating population density, which dictates how many people can live in any given area, or how many stories a building is allowed to be."
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#questions",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#questions",
    "title": "Iowa Zoning Guide Week One",
    "section": "Questions",
    "text": "Questions\n\n\nWhat is the relationship between housing density and housing costs?\n\n\nTo answer this question, we will look at the housing density based on the zoning code of a given city. We would then combine that data with the data we have on the average price per house in those zones. Creating a graph where housing density is on the x-axis, housing prices are on the y-axis, and each city is a different color would result in a graph that conveys the relationship. We would likely need to only look at the cities that have more than 2 residential districts, as those district types likely wouldn’t provide a clear relationship, given the limited data they would provide.\n\n\n\n\nWhat is zoning’s effect on transportation times/costs?\n\n\nDoes commute time affect purchase rate and/or price? If so, how?\n\n\nHow does the cost of living in the suburbs with decreased land prices compare to the cost of commuting to work?\n\n\nHow does the number of unsold homes relate to the zoning area? (Ex: On average, are there more unsold homes in Low Density Residential Zones?)"
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#cogs",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#cogs",
    "title": "Iowa Zoning Guide Week One",
    "section": "COGS",
    "text": "COGS\nIowa has Councils of Government (COGs) which contain different counties. Unfortunately, CIRTPA was somewhat recently dissolved, so we’re ignoring that change."
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#data-wrangling",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#data-wrangling",
    "title": "Iowa Zoning Guide Week One",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nCities\n\nBoone\n\n\nAdel\n\n\nGrimes\n\n\nJohnson\n\n\nAnkeny\n\n\nWest Des Moines\n\n\nIndianola\n\n\nPella"
  },
  {
    "objectID": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#form-snippet",
    "href": "blogs2024/Team_Blog_One/weekly-blog-one-iowa-zoning-guide/Weekly_Team_Blog.html#form-snippet",
    "title": "Iowa Zoning Guide Week One",
    "section": "Form Snippet",
    "text": "Form Snippet\nContains information about:\n\nDistrict type\n\n\nPermitted housing\n\n\nDetailed information of housing dimensions"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Public Good 2024",
    "section": "",
    "text": "This Quarto website is an experimental page hosting project documentation for the 2024 Data Science for Public Good program at Iowa State University. The Project Blogs tab contains the documentation for each team’s work on a weekly basis. The Student Blogs tab contains the blog posts of all individual participants in DSPG for the summer of 2024.\nTo learn more about the program, follow the link to the Iowa State University DSPG Program."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Data Science for Public Good 2024",
    "section": "",
    "text": "This Quarto website is an experimental page hosting project documentation for the 2024 Data Science for Public Good program at Iowa State University. The Project Blogs tab contains the documentation for each team’s work on a weekly basis. The Student Blogs tab contains the blog posts of all individual participants in DSPG for the summer of 2024.\nTo learn more about the program, follow the link to the Iowa State University DSPG Program."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Data Science for Public Good 2024",
    "section": "Projects",
    "text": "Projects\n\nIowa Zoning Guide\nDescription to be placed here\nProject Sponsor: IEDA\nProject Advisers: Jay Maxwell, Chris Seeger\nGraduate Fellows: Tabassum Zaman\nUndergraduate Scholars: Joseph Betti, Blake Underwood, Naomi Mauss\nFinal Presentation Teaser:\nGitHub: Repository\nProject Blog Pages\nFinal Presentation Video: YouTube\n\n\nCommunity Capitals\nDescription to be placed here\nProject Sponsor: Sponsor\nProject Advisers: Matthew Voss\nGraduate Fellows: Solomon Eshun\nUndergraduate Scholars: Manjul Balayar, Lily Christenson, Anh Le\nFinal Presentation Teaser:\nGitHub: Repository\nProject Blog Pages\nFinal Presentation Video: YouTube\n\n\nHousing and AI\nDescription to be placed here\nProject Sponsor: AI Institute for Resilient Agriculture (AIIRA)\nProject Advisers: Liesl Eathington, Rakesh Shah\nGraduate Fellows: Karthik Hanumanthaiah\nUndergraduate Scholars: Neha Tirunagiri, Cael Leistikow, Alister Gan\nFinal Presentation Teaser:\nGitHub: Repository\nProject Blog Pages\nFinal Presentation Video: YouTube"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Quarto Guides\n\n\n\n\nOverleaf LateX Guide"
  },
  {
    "objectID": "resources.html#quarto",
    "href": "resources.html#quarto",
    "title": "Resources",
    "section": "",
    "text": "Quarto Guides"
  },
  {
    "objectID": "resources.html#latex",
    "href": "resources.html#latex",
    "title": "Resources",
    "section": "",
    "text": "Overleaf LateX Guide"
  },
  {
    "objectID": "resources.html#packages-and-modules",
    "href": "resources.html#packages-and-modules",
    "title": "Resources",
    "section": "Packages and Modules",
    "text": "Packages and Modules"
  },
  {
    "objectID": "resources.html#data-manipulation",
    "href": "resources.html#data-manipulation",
    "title": "Resources",
    "section": "Data Manipulation",
    "text": "Data Manipulation\nPandas\ndask"
  },
  {
    "objectID": "resources.html#visualization",
    "href": "resources.html#visualization",
    "title": "Resources",
    "section": "Visualization",
    "text": "Visualization\nMatplotlib\nSeaborn"
  },
  {
    "objectID": "resources.html#books",
    "href": "resources.html#books",
    "title": "Resources",
    "section": "Books",
    "text": "Books\nR for Data Science\nAdvanced R\nAdvanced R Solutions\nR Graphics Cookbook\nElegant Graphics for Data Analysis\nMastering R Shiny"
  },
  {
    "objectID": "resources.html#packages",
    "href": "resources.html#packages",
    "title": "Resources",
    "section": "Packages",
    "text": "Packages\nPackage Cheatsheets\n\n\nData Manipulation\ndplyr\ntidycensus\nstringr\nreadr\nscales\npurrr\nfurrr\n\n\n\nVisualizations\nggplot2\n\n\n\nTable Manipulation\nGT\nflextable\n\n\n\nSpatial\nsf\nleaflet\nmapview\n\n\n\nDashboarding\nshiny\nflexdashboard\n\n\n\nPython Integration\nReticulate"
  },
  {
    "objectID": "resources.html#tidycensus",
    "href": "resources.html#tidycensus",
    "title": "Resources",
    "section": "tidycensus",
    "text": "tidycensus\nAnalyzing US Census Data\nWorking with the 2022 ACS\nAnalyzing 2020 Decennial US Census Data in R\nDoing ‘GIS’ and making maps with US Census data in R"
  },
  {
    "objectID": "resources.html#census-information",
    "href": "resources.html#census-information",
    "title": "Resources",
    "section": "Census Information",
    "text": "Census Information\nCensus Reporter\nCensus Bureau Handbooks for Data Users\nAmerican Community Survey (ACS) Handbook\nAmerican Community Survey (ACS) Documentation\nPublic Use Microdata Sample (PuMS) Handbook\nPublic Use Microdata Sample (PuMS) Documentation\nIPUMS PUMS Access"
  },
  {
    "objectID": "resources.html#books-1",
    "href": "resources.html#books-1",
    "title": "Resources",
    "section": "Books",
    "text": "Books\nCausality for Machine Learning"
  },
  {
    "objectID": "studentBlogs/alister.html",
    "href": "studentBlogs/alister.html",
    "title": "Alister Gan",
    "section": "",
    "text": "Alister’s Midpoint Blog\n\n\n\n\n\n\nMidpoint Reflection\n\n\n\n\n\n\n\n\n\nJun 15, 2024\n\n\nAlister Gan\n\n\n\n\n\n\n\n\n\n\n\n\nAlister’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nAlister Gan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/blake.html",
    "href": "studentBlogs/blake.html",
    "title": "Blake Underwood",
    "section": "",
    "text": "Blake’s Midpoint Reflection\n\n\n\n\n\n\nMidpoint Reflection\n\n\n\n\n\n\n\n\n\nJan 6, 2026\n\n\nBlake Underwood\n\n\n\n\n\n\n\n\n\n\n\n\nBlake’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nBlake Underwood\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/cael/student-introductions/Cael_Introduction.html",
    "href": "studentBlogs/cael/student-introductions/Cael_Introduction.html",
    "title": "Cael’s Introduction",
    "section": "",
    "text": "Hello, My name is Cael Leistikow\n\nWhere are you from?\nI grew up in Frederika, Iowa which is about 30 minutes north of Waterloo and I attended New Hampton High School.\n\n\nWhat’s your current academic year?\nI am currently a Senior at Iowa State going into my fourth year with intentions of graduating next Spring.\n\n\nWhat degrees are you studying?\nI am majoring in Data science with a minor in Economics\n\n\nWhy did you choose your degrees?\nI chose these degree because I had always been fascinated by statistics, especially sports statistics, and as I went through high school I began to grow an interest in programming as well and when I discovered that Iowa State offered a degree that was a mixture of the two, I knew that it was what I wanted to pursue.\n\n\nWhat are your plans with your degree(s)?\nMy plans for after college is to use my degrees as a Data Analyst or Data Scientist within the business sector and having my Economics degree as well to be able to apply my knowledge towards making informative business decisions. After a few years I would consider to go for my dream job of working as a sports data analyst for a professional US sports team, preferably in baseball or football.\n\n\nWhy do you care about data science?\nI care about data science cause I firmly believe that it is the future towards optimizing and maximizing everything that we do in society. Everything that we do can be put into data and being able to understand how to work with and interpret that data is very valuable. Although it comes with a lot of power that can be manipulated if the owner so desires.\n\n\n\nDo you have any prior experience with data science?\nI have taken numerous data science courses here at Iowa State. I have had an internship with Kingland Systems as a Data Research Analyst. Finally, I have been a Statistician when I was playing under levels of athletics for the varsity team in high school.\n\n\nWhat kinds of extracurricular activities/hobbies do you care about?\nMy extracurricular activities are the Sports Analytics Club, Data Science Club, and Intramural sports.\nMy favorite hobbies are playing basketball, watching sports and going to games, hanging out with friends, listening to music, cooking, and travel.\n\n\nWhat do you want your peers to know about you?\n\nAn interesting fact about me is I have visited 24 of the 50 states as well as DC and Canada. Obviously I like to travel. My favorite trip being to Colorado. Additionally, my most desired place to travel next would be Hawaii as I haven’t gone outside on the intercontinental US or Canada.\n\n\nList some skills in data science that you feel you do well?\nI feel I do very well in data visualization, statistical analysis, machine learning/modeling, R programming as it is my more preferred programming language, and finally SQL and database management.\n\n\nList at least three skills you would like to learn or develop during the DSPG program.\nI would like to develop during the DSPG program is AI development as AI is going to continue to integrated itself in everyday life. Another skill I would like to learn is GIS as I have seen a lot of employers looking for candidates who are able to effectively use GIS. Finally, the last thing I would like to learn is how the data science process works in a real-world scenario that I could then apply in the future.\n\n\nWhy are you pursuing these skills? Why do they matter to you?\nThe reason I’m pursuing these skills and why the matter to me is because I want to be able to put myself in the best position I can for when I’m beginning to look for my full-time career. Being a senior this coming year I need to refine and learn any skills that I can to help me stand out as a candidate to make sure I position myself for the job I want and to have a successful life.\n\n\nHow do you hope to improve these skills?\nI hope to learn and improve these skills through Datacamp as a tutorial for GIS or AI to help create a foundational understanding and then from there apply it to the projects to help use this knowledge on a real-world issue. Additionally, learning from the project leaders and the lunch talks during the program will help me see from other people’s perspective and ask questions to grow my understanding further."
  },
  {
    "objectID": "studentBlogs/harun/student-introductions/harun_Introduction.html",
    "href": "studentBlogs/harun/student-introductions/harun_Introduction.html",
    "title": "Harun’s Introduction",
    "section": "",
    "text": "My name is Harun Celik and I am an international student from Turkey getting my PhD Degree in History. History and data science might seem like polar opposites to a lot of people but for me, the disciplines not only intersect at interesting places, but compliment each other well. I find that historians are particularly good at viewing phenomena from multiple perspectives, and data scientists are particularly good at presenting data from different vantage points. These things are certainly similar but not the same.\n\n\n\nA Very Descriptive Venn Diagram\n\n\nI’m doing my PhD because I like to experiment. In particular, I care about experimenting with interdisciplinary methodologies to help approach questions in social disciplines by providing a different angle than what’s traditionally taken. I am pursuing my degree as a way to learn about the different ways that I can approach subjects I care deeply about. These include…\n\nDeveloping pedagogy around The Use of Technology in the Social Sciences.\nUsing Research to Develop Programs that Practice Experiential Learning.\nExperimenting with Data Science and Spatial Technology on Historical Data.\nReading about the Historical Development of Our Contemporary Urban Landscape."
  },
  {
    "objectID": "studentBlogs/harun/student-introductions/harun_Introduction.html#about-me",
    "href": "studentBlogs/harun/student-introductions/harun_Introduction.html#about-me",
    "title": "Harun’s Introduction",
    "section": "",
    "text": "My name is Harun Celik and I am an international student from Turkey getting my PhD Degree in History. History and data science might seem like polar opposites to a lot of people but for me, the disciplines not only intersect at interesting places, but compliment each other well. I find that historians are particularly good at viewing phenomena from multiple perspectives, and data scientists are particularly good at presenting data from different vantage points. These things are certainly similar but not the same.\n\n\n\nA Very Descriptive Venn Diagram\n\n\nI’m doing my PhD because I like to experiment. In particular, I care about experimenting with interdisciplinary methodologies to help approach questions in social disciplines by providing a different angle than what’s traditionally taken. I am pursuing my degree as a way to learn about the different ways that I can approach subjects I care deeply about. These include…\n\nDeveloping pedagogy around The Use of Technology in the Social Sciences.\nUsing Research to Develop Programs that Practice Experiential Learning.\nExperimenting with Data Science and Spatial Technology on Historical Data.\nReading about the Historical Development of Our Contemporary Urban Landscape."
  },
  {
    "objectID": "studentBlogs/harun/student-introductions/harun_Introduction.html#my-dspg-experience",
    "href": "studentBlogs/harun/student-introductions/harun_Introduction.html#my-dspg-experience",
    "title": "Harun’s Introduction",
    "section": "My DSPG Experience",
    "text": "My DSPG Experience\nAs an additional degree, I’ve completed the Graduate GIS Certificate Program at ISU and sought for ways to practice these skills on projects that had some public influence. This is how I stumbled across the DSPG program and applied to be a graduate fellow for summer of 2022. I enjoyed the experience so much that I wanted to rejoin the following summer in 2023 with a couple of additional responsibilities. After sharing my ideas with the Community and Economic Development Extension team, I was lucky to get an assistantship that focused on improving the DSPG program with the responses we’ve received from past participants through our surveys.\nI really enjoy the DSPG program for a multitude of reasons, but here a couple of my favorites.\n\nThe program supports communal and personal learning.\n\nParticipants to the program come from varying levels of education, from first year students to PhD candidates. This usually means that everyone comes in at different skill levels and uses that to either develop the skills that they already know or learn new ones.\n\nProjects are quite varied in their goals and flexible for hands-on learning\n\nDifferent clients expect different results and for a lot of the projects, it is the exploration conducted by students that shapes the final deliverable of the projects.\n\nThe program address real questions by clients interested in learning from data.\n\nThe DSPG projects address complex and real questions about the social world making the application of data science skills extremely practical for real-world scenarios."
  },
  {
    "objectID": "studentBlogs/harun/student-introductions/harun_Introduction.html#my-dspg-expectations-this-year",
    "href": "studentBlogs/harun/student-introductions/harun_Introduction.html#my-dspg-expectations-this-year",
    "title": "Harun’s Introduction",
    "section": "My DSPG Expectations This Year",
    "text": "My DSPG Expectations This Year\nBeing the coordinator of the program for the first time, my main objectives are to make sure that the DSPG experience is enjoyable and informative for everyone involved. I have yet to solidify all my thoughts but here is a mindmap of what I expect will be my top three goals for this summer.\n\n\n\nMindmap of Harun’s DSPG Objectives"
  },
  {
    "objectID": "studentBlogs/harun/student-introductions/harun_Introduction.html#fun-fact",
    "href": "studentBlogs/harun/student-introductions/harun_Introduction.html#fun-fact",
    "title": "Harun’s Introduction",
    "section": "Fun Fact",
    "text": "Fun Fact\nI play the bass guitar and I love to explore different genres of music, especially ones where the bass guitar has a prominent or unique role. I’d be happy to take any recommendations if you have any favorite albums or songs.\nLately I’ve been listening to Lugares Comunes by Inti-Illimari."
  },
  {
    "objectID": "studentBlogs/joseph.html",
    "href": "studentBlogs/joseph.html",
    "title": "Joseph Betti",
    "section": "",
    "text": "Joseph’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nJoseph Betti\n\n\n\n\n\n\n\n\n\n\n\n\nJoseph Betti’s Midpoint Reflection\n\n\n\n\n\n\nMidpoint Reflection\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nJoseph Betti\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html",
    "href": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html",
    "title": "Joseph Betti’s Midpoint Reflection",
    "section": "",
    "text": "My goals at the beginning of the summer:\n\nImprove at GIS\nLearn more about statistics\nLearn how to program (R, Python, SQL, etc.)\nLearn how to apply data science as a Landscape Architect\nGain real-world experience"
  },
  {
    "objectID": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html#review-of-my-goals",
    "href": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html#review-of-my-goals",
    "title": "Joseph Betti’s Midpoint Reflection",
    "section": "",
    "text": "My goals at the beginning of the summer:\n\nImprove at GIS\nLearn more about statistics\nLearn how to program (R, Python, SQL, etc.)\nLearn how to apply data science as a Landscape Architect\nGain real-world experience"
  },
  {
    "objectID": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html#progress",
    "href": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html#progress",
    "title": "Joseph Betti’s Midpoint Reflection",
    "section": "Progress",
    "text": "Progress\n\nGIS\n\n\nI feel as if I have gained many new tools to help me as a Landscape Architect. The largest benefit is the ability to access information from the census.\n\nList of Improvements\n\nUsing census data\nWorking with new file types\nNew ways of manipulating attributes (in ArcGIS Pro and R)\nMaking maps in R\n\n\n\n\nStatistics\nI have not had the opportunity to learn much about statistics. I believe this knowledge will come later when my group and I dive deeper into the analysis of zoning. However, I have improved in visualizing data.\n\n\nProgramming\nThe DSPG program has greatly improved my programming skills in R and python. I have also renewed my long-forgotten interest in making games outside of work. \n\n\nList of Improvements\n\nGathering census data\nAnalyzing data\nVisualizations and mapping\nMaking games"
  },
  {
    "objectID": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html#data-science-for-landscape-architecture",
    "href": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html#data-science-for-landscape-architecture",
    "title": "Joseph Betti’s Midpoint Reflection",
    "section": "Data Science for Landscape Architecture",
    "text": "Data Science for Landscape Architecture\n\n\nI believe that this experience will improve my skills as a Landscape Architect. I now have more tools to understand the people that I am designing for, as well as the conditions of their environment.\n\nPotential Uses\n\nFinding data\nUnderstanding demographics\nFinding areas of suitability\nData visualization"
  },
  {
    "objectID": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html#real-world-experience",
    "href": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html#real-world-experience",
    "title": "Joseph Betti’s Midpoint Reflection",
    "section": "Real-World Experience",
    "text": "Real-World Experience\nThe zoning guide is a statewide project that has real implications on the way Iowa cities make zoning laws. The work we are doing has the potential to shape cities and lower housing costs, affecting the quality of life for many Iowa citizens. The real-world nature of the project certainly adds pressure, but it is satisfying to be making an impact on the state. Experiencing a real-world project has also given me a better understanding of how cities organize themselves"
  },
  {
    "objectID": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html#goals-for-the-upcoming-weeks",
    "href": "studentBlogs/joseph/student-midpoint-blog-jsbetti/Joseph_Midpoint_Blog.html#goals-for-the-upcoming-weeks",
    "title": "Joseph Betti’s Midpoint Reflection",
    "section": "Goals for the Upcoming Weeks",
    "text": "Goals for the Upcoming Weeks\n\nImprove at programming\nGain a greater perspective on the intentions and effects of zoning\nImprove at data collection and analysis\nImprove critical thinking skills\nImprove in ArcGIS Pro"
  },
  {
    "objectID": "studentBlogs/karthik/student-introductions/Karthik.html",
    "href": "studentBlogs/karthik/student-introductions/Karthik.html",
    "title": "Karthik’s Introduction",
    "section": "",
    "text": "I’m from India and I’m currently pursuing my Master’s in Department of Computer Science, My research work is on Formal Verification & Machine Learning."
  },
  {
    "objectID": "studentBlogs/karthik/student-introductions/Karthik.html#my-work",
    "href": "studentBlogs/karthik/student-introductions/Karthik.html#my-work",
    "title": "Karthik’s Introduction",
    "section": "My Work",
    "text": "My Work\nIf you have some thoughts on how to verify some unknown mathematical function that works correctly, I would be happy to share my thoughts. If you sometimes wonder how to hack into a Machine learning model, I can also add something to it."
  },
  {
    "objectID": "studentBlogs/karthik/student-introductions/Karthik.html#whats-up-with-dspg",
    "href": "studentBlogs/karthik/student-introductions/Karthik.html#whats-up-with-dspg",
    "title": "Karthik’s Introduction",
    "section": "What’s up with DSPG?",
    "text": "What’s up with DSPG?\nI like solving problems, so far data science has a lot of open problems and I see potential in data science solving a lot of real-world problems. I have designed software applications in the past, but this time with DSPG I’m planning to gain some insights into designing mathematical models for the data."
  },
  {
    "objectID": "studentBlogs/karthik/student-introductions/Karthik.html#hobby",
    "href": "studentBlogs/karthik/student-introductions/Karthik.html#hobby",
    "title": "Karthik’s Introduction",
    "section": "Hobby",
    "text": "Hobby"
  },
  {
    "objectID": "studentBlogs/karthik/student-introductions/Karthik.html#fun-facts",
    "href": "studentBlogs/karthik/student-introductions/Karthik.html#fun-facts",
    "title": "Karthik’s Introduction",
    "section": "Fun Facts",
    "text": "Fun Facts\nRecently I got to know (credits to Tabassum), the word “karthik” closely relates to “Season & Farming” and I love Farming, Fun stuff!"
  },
  {
    "objectID": "studentBlogs/manjul.html",
    "href": "studentBlogs/manjul.html",
    "title": "Manjul Balayar",
    "section": "",
    "text": "Manjul’s Midpoint Reflection\n\n\n\n\n\n\nMidpoint Reflection\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\nManjul Balayar\n\n\n\n\n\n\n\n\n\n\n\n\nManjul’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nManjul Balayar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/naomi.html",
    "href": "studentBlogs/naomi.html",
    "title": "Naomi Mauss",
    "section": "",
    "text": "Naomi Mauss’ Midpoint Reflection\n\n\n\n\n\n\nMidpoint Reflection\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\nNaomi Mauss\n\n\n\n\n\n\n\n\n\n\n\n\nNaomi’s Introduction\n\n\n\n\n\n\nnews\n\n\n\nLearn about me here.\n\n\n\n\n\nMay 16, 2024\n\n\nNaomi Mauss\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html",
    "href": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html",
    "title": "Naomi Mauss’ Midpoint Reflection",
    "section": "",
    "text": "One of my goals was to learn about the data science industry and the role I could hold in the industry. In DSPG, I have learned about the different jobs people have in mapping cities, from creating and maintaining zoning codes to making GIS maps to organizing census data. During one of the presentations regarding how to collect and use data from a city, I learned about a program where government employees can gain further education while working for the government. I am intrigued by these opportunities, as well as other opportunities to work in data science about which I have not yet learned.\nIn terms of technical skills, I have learned more about visualizing data with ggplot and GIS mapping. I am currently working on writing code to web scrape JSON data from a realty website called RedFin. Finally, I have learned more about cleaning data so that it represents information in both a digestible and accurate manner.\n\nOne example of the work I’ve done is plotting vacant housing units in Iowa"
  },
  {
    "objectID": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html#my-goals",
    "href": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html#my-goals",
    "title": "Naomi Mauss’ Midpoint Reflection",
    "section": "",
    "text": "One of my goals was to learn about the data science industry and the role I could hold in the industry. In DSPG, I have learned about the different jobs people have in mapping cities, from creating and maintaining zoning codes to making GIS maps to organizing census data. During one of the presentations regarding how to collect and use data from a city, I learned about a program where government employees can gain further education while working for the government. I am intrigued by these opportunities, as well as other opportunities to work in data science about which I have not yet learned.\nIn terms of technical skills, I have learned more about visualizing data with ggplot and GIS mapping. I am currently working on writing code to web scrape JSON data from a realty website called RedFin. Finally, I have learned more about cleaning data so that it represents information in both a digestible and accurate manner.\n\nOne example of the work I’ve done is plotting vacant housing units in Iowa"
  },
  {
    "objectID": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html#plotting-vacant-housing-units",
    "href": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html#plotting-vacant-housing-units",
    "title": "Naomi Mauss’ Midpoint Reflection",
    "section": "Plotting vacant housing units",
    "text": "Plotting vacant housing units\n\nFirst, I plotted vacant housing data for all 330 target places we want to track in Iowa.\n\n\n\nI then calculated the acceptable Margin of Error (MOE) for vacant housing units, and plotted those places.\n\n\n\nGiven that there was extremely limited data on vacant housing units by place with an acceptable MOE, I then found the county data on vacant housing units with an acceptable MOE. This time, there were only three counties with unacceptable MOEs."
  },
  {
    "objectID": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html#looking-back-at-earlier-goals",
    "href": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html#looking-back-at-earlier-goals",
    "title": "Naomi Mauss’ Midpoint Reflection",
    "section": "Looking back at earlier goals",
    "text": "Looking back at earlier goals\n\nI’ve been able to learn more about R and Python. However, I’m also spending time learning about zoning legislation and how zoning code affects many different facets of life. I have a new appreciation for the significance of zoning. I hope to keep improving upon the skills I’m currently learning.\nI don’t think I’m meeting all the goals I had at the beginning of the program, but I don’t think that’s necessarily a bad thing. I’m doing much more productive things than I thought I would be able to acheive, and I’m learning more about a wider variety of topics. Specifically, I know much more about zoning and data collection than I had before, and I’m confident that’s knowledge I’ll be able to use later in my career."
  },
  {
    "objectID": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html#what-have-you-been-learning-so-far",
    "href": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html#what-have-you-been-learning-so-far",
    "title": "Naomi Mauss’ Midpoint Reflection",
    "section": "What have you been learning so far?",
    "text": "What have you been learning so far?\n\nOne of my goals was to learn about the data science industry and the role I could hold in the industry. In DSPG, I have learned about the different jobs people have in mapping cities, from creating and maintaining zoning codes to making GIS maps to organizing census data. During one of the presentations regarding how to collect and use data from a city, I learned about a program where government employees can gain further education while working for the government. I am intrigued by these opportunities, as well as other opportunities to work in data science about which I have not yet learned.\nIn terms of technical skills, I have learned more about visualizing data with ggplot and GIS mapping. I am currently working on writing code to web scrape JSON data from a realtor website called RedFin. Finally, I have learned more about cleaning data so that it represents information in both a digestible and accurate manner."
  },
  {
    "objectID": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html#working-with-my-team",
    "href": "studentBlogs/naomi/student-midpoint-blog-bootingabend/NaomiMauss_Midpoint_Blog.html#working-with-my-team",
    "title": "Naomi Mauss’ Midpoint Reflection",
    "section": "Working with my team",
    "text": "Working with my team\nI am so grateful to be working with the other talented people on the DSPG team. They have helped me grow and learn as we share the things we know about data science and share our knowledge. Thank you Tabassum, Blake, and Joe for working with me this summer!"
  },
  {
    "objectID": "studentBlogs/nhat.html",
    "href": "studentBlogs/nhat.html",
    "title": "Nhat Le",
    "section": "",
    "text": "Nhat’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nNhat Le\n\n\n\n\n\n\n\n\n\n\n\n\nChris’ Midpoint Reflection\n\n\n\n\n\n\nMidpoint Reflection\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nChris\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/solomon.html",
    "href": "studentBlogs/solomon.html",
    "title": "Solomon Eshun",
    "section": "",
    "text": "Solomon’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\nSolomon Eshun\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "studentBlogs/tabassum.html",
    "href": "studentBlogs/tabassum.html",
    "title": "Tabassum Zaman",
    "section": "",
    "text": "Tabassum’s Introduction\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMay 17, 2024\n\n\nTabassum Zaman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html",
    "title": "Iowa Zoning Guide Week Three",
    "section": "",
    "text": "First, I plotted vacant housing data for all 330 target places we want to track in Iowa.\n\n\n\nI then calculated the acceptable Margin of Error (MOE) for vacant housing units, and plotted those places.\n\n\n\nGiven that there was extremely limited data on vacant housing units by place with an acceptable MOE, I then found the county data on vacant housing units with an acceptable MOE. This time, there were only three counties with unacceptable MOEs."
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#plotting-vacant-housing-units",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#plotting-vacant-housing-units",
    "title": "Iowa Zoning Guide Week Three",
    "section": "",
    "text": "First, I plotted vacant housing data for all 330 target places we want to track in Iowa.\n\n\n\nI then calculated the acceptable Margin of Error (MOE) for vacant housing units, and plotted those places.\n\n\n\nGiven that there was extremely limited data on vacant housing units by place with an acceptable MOE, I then found the county data on vacant housing units with an acceptable MOE. This time, there were only three counties with unacceptable MOEs."
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#redfin-data",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#redfin-data",
    "title": "Iowa Zoning Guide Week Three",
    "section": "Redfin Data",
    "text": "Redfin Data\n\nAfter spending time looking for and web scraping Redfin data, I was able to access a 6.8 GB document on realtor data in the US by place (city) between 2012 and May 2024.\n\n\nThis data is good to have, so now I’m working on cleaning the data.\n\n\n\nI am using Sublime Text to view and edit this document. Sublime Text is specifically designed to handle large text files, which makes this a good editor for this specific dataset. Between VS Code, Excel, and RStudio, this is the only editor that did not immediately crash when I tried to load the file. It did take approximately 3 hours to load, but overall, I am happy with this editor’s capabilities."
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#encoding",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#encoding",
    "title": "Iowa Zoning Guide Week Three",
    "section": "Encoding",
    "text": "Encoding\nWe have encoded 25 cities completely as of 6/14/2024.\n\n42% of CIRTPA\n8% of Iowa cities included in the zoning guide\n.25% of cities in the world*\n\nAt this rate, we would finish encoding the world in 23 years.\n*Cities in the world defined as having a population of over 50,000. We are not using that definition for cities in Iowa."
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#collecting-gis-data",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#collecting-gis-data",
    "title": "Iowa Zoning Guide Week Three",
    "section": "Collecting GIS Data",
    "text": "Collecting GIS Data\nThis week, we gathered basic data for future analysis in ArcGIS Pro. As of now, we can use this information to make comparisons between cities and gain a general overview of city demographics and economics. The data collected is from the ACS.\n\nData Collected\n\n\n\nEmployment status\nArmed Forces status\nMedian income\nMedian age by sex\nPopulation\nMedian Gross Rent\nMedian monthly housing costs\n\nFor those with and without mortgages\n\nHousing occupancy status\n\n\nExamples:"
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#beginning-stages-of-a-housing-analyses.",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#beginning-stages-of-a-housing-analyses.",
    "title": "Iowa Zoning Guide Week Three",
    "section": "beginning stages of a housing analyses.",
    "text": "beginning stages of a housing analyses."
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#lets-take-a-look-at-the-future-for-ames-home-owners.",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#lets-take-a-look-at-the-future-for-ames-home-owners.",
    "title": "Iowa Zoning Guide Week Three",
    "section": "Let’s take a look at the future for Ames home-owners.",
    "text": "Let’s take a look at the future for Ames home-owners.\nVideo\n\nIt appears that suburban areas of Ames have been growing faster in household income then “urban” area’s. But this analysis begs a new question, is the cost of rents increasing in the same area’s? (later will calculate cost of living)\n\nVideo\n\nWe see a different picture when it comes to rent cost. Rent costs seemed to grow evenly across Ames from 2010 - 2020, but 2020-2022 costs around Iowa State have increased a noticeably faster rate then the suburban areas… I started college in 2020!\n\n\nThese two trends bring up many questions about the Ames renting market.\n\n\nA question this may bring up “what is Ames and household income going to look like in 2030?”\n\nVideo\n\nBy looking at my household income forecast map we can see that its likely that the suburban area’s will continue to grow at a faster rate than areas around ISU."
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#where-to-from-here",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-iowa-zoning-guide/Weekly_Team_Blog.html#where-to-from-here",
    "title": "Iowa Zoning Guide Week Three",
    "section": "Where to from here?",
    "text": "Where to from here?\n\nThis is just a start of a much needed furthur analysis of the city of Ames housing market, but I will create a forecast rent costs map that goes tell year 2030, in order to continue to compare these two variables. I also plan to find an value for overall cost of living and overall personal income and use those variables to determine what area’s will be affordable in the future and what area’s will likely not be affordable for the average person."
  },
  {
    "objectID": "studentBlogs/lily/student-midpoint-blog-lilychristenson/Lily's_Midpoint_Blog.html",
    "href": "studentBlogs/lily/student-midpoint-blog-lilychristenson/Lily's_Midpoint_Blog.html",
    "title": "Lily’s Midpoint Reflection",
    "section": "",
    "text": "What I have been learning so far\nI have grown my coding skills in the first few weeks of the DSPG Program. We first started with a few courses on DataCamp. It gave me a good foundation for the type of coding that I would be using. I got assigned to the Capitals project, and my team and I had our first two meetings with our clients. We were assigned to focus on Social and Cultural Capital. I had to read some articles before I started my research on Social and Cultural Capital. Throughout my research, I found that Cultural measures were slightly more difficult to find than Social measures. I had to think outside the box for new, creative ways to measure Cultural measures.\nI have gained skills in data cleaning, visualizations, and client meetings. When it comes to cleaning a dataset, you have to be very careful and observant. It is always important to keep a copy of the original dataset just in case something were to go wrong when cleaning. I worked with employment shock datasets. While cleaning the datasets, I realized there were duplicate rows. I had to look closer at the dataset to determine the best way to eliminate them. The only difference between the duplicates was that some points didn’t have information in the industry column, whereas others did. As I was going through that process, I also looked at the unique variables. I found that some names represented the same industry or city but were slightly named differently. In another dataset I have worked with, multiple rows were not needed. The rows had the same beginning so I learned how to get rid of rows based on the unique character that the rows had.\n\n\nCode and Visualization\n\nThis wordcloud shows which cities in Iowa had the most WARN employment shock cases from 2005 - 2024.\n\nThis lollipop plot shows the number of employees who lost their jobs from the year 2005 - 2024.\n\nThis lollipop plot shows how a notice type impacted counties in Iowa.\n\nThis treemap shows the Simpson Index race for cities in Iowa. Blue represents the largest race diversity, and orange represents the smallest.\n\nThis map shows the public art less than 50 per 10,000 in cities in Iowa.\n\n\nNew directions of learning\nThroughout the program, I have been looking into new ways to visualize data. I think it’s important to know many different ways to visualize data. Along with that, it is equally important to know when and where to use which type of visualization. For example, it is important to know if you should use a bar chart, pie chart, wordcloud, time plot, map, etc.\n\n\nSkills I hope to improve for the rest of the program\nI want to continue to improve my data cleaning, visualization, and group weekly blog. I hope to keep improving my coding skills in data cleaning and creating new types of visualizations by finding new, efficient ways to code. I also want to figure out better ways for my group to work on the weekly blog posts. We have figured out the dependencies part and now we need to be more aware of the codes that we want to run.\n\n\nGoals met?\nI believe I am meeting the goals that I outlined at the start of the program. I am improving my coding skills every day, building up skills for client meetings, getting more comfortable speaking in front of a group, and trying new coding approaches.\nCommunity Capitals Team and Clients:"
  },
  {
    "objectID": "studentBlogs/blake/student-introductions/Blake_Introduction.html",
    "href": "studentBlogs/blake/student-introductions/Blake_Introduction.html",
    "title": "Blake’s Introduction",
    "section": "",
    "text": "this is me"
  },
  {
    "objectID": "studentBlogs/blake/student-introductions/Blake_Introduction.html#part-2",
    "href": "studentBlogs/blake/student-introductions/Blake_Introduction.html#part-2",
    "title": "Blake’s Introduction",
    "section": "Part 2",
    "text": "Part 2\n\n\nWhat skills do you value and appreciate about yourself?\n\n\n\nI really value my guitar skills and my coding skills.\n\n\n\nI’ve spent so many hours in the last couple years playing guitar trying to learn every song I enjoy listening to.\nPre chatgpt being popular I spent alot of time learning to code and do multiple coding online courses and like 200 coding problems on Leetcode, looking back now most of what I learned became irrelevant due to chat gpt but I still developed a skill that has been deteriorating the more I use GPT unfortunately.\n\n\n\n\nDS skills\n\n\n\nI scored “The thinker” on the personality test we all took on day one, and I do think that represents me well. I enjoy coming up with new ideas for solving problems. Not giving up on finding actually meaningful results that can have a postive impact.\n\n\n\nthree skills you want to develop\n\n\n\nPython and R are extremely important skills for data science and I have been working with the long enough to know how valuable they can be. I also really like using Tableau, I think it’s popular for a reason, for being as high level as it is it can do some pretty complex things such as being able to use SQL to manipulate data while in tableau.\n\n\n\nwhy these skills?\n\n\n\nI’m pursuing these skills because I believe they are the best tools for data sciences to become successful. It’s a safer bet to say that in Python and R will still be relevant in 10 years but some of the higher level skills may be replaces by ai or somethings even higher level.\n\n\n\nhow are you going to improve skills?\n\n\n\nWork on them everyday. The more I work with these technologies the more comfortable I will get with them overtime and the faster I can get things done with them."
  },
  {
    "objectID": "studentBlogs/alister/student-midpoint-blog-Alister03USA/Alister_Midpoint_Blog.html#learning-journey",
    "href": "studentBlogs/alister/student-midpoint-blog-Alister03USA/Alister_Midpoint_Blog.html#learning-journey",
    "title": "Alister’s Midpoint Blog",
    "section": "Learning Journey",
    "text": "Learning Journey\n\nAs I reflect back to my first 5 weeks of DSPG, little did I realized how much technical and non-technical knowledge that I have been able to grasp in this short period of time. And the lists below are some of the pieces of my learning journey thus far:\n\n\nData Exploration\n\n\nComing into DSPG, I do not have much experience in looking through raw data and sourcing through the Internet for suitable data. Being part of DSPG has opened up opportunities for me to hone my skill in data sourcing. I was also introduced to various platforms such as the U.S Census Bureau Data, Census Reporter and etc to look for datasets that best fit my project. My biggest realization from data sourcing is that it is one of the most crucial skill as having a credible and suitable dataset is the foundation to a successful data science project.\n\n\n\n\nAge Distribution For Blocks In Des Moines\n\n\n\n\n\nAge Distribution Across Block Groups In Polk County\n\n\n\nData Analysis and Data Visualization through R\n\n\nComing into DSPG, I have not yet had the chance to learn R but throughout the program, I had the chance to go through several R courses and most importantly able to have first hand experience in using R for data analysis and data visualization, specifically on the data from Decennial and American Community Survey (ACS). The example below is one of the data extraction process that I did by taking the decennial data of the number of seniors in the Des Moines Blocks and using the data to plot a mapview of the distribution of total seniors across different blocks.\n\n\npopulation65AndAbove &lt;- get_decennial(\n  geography = \"block\",\n  variables = c(Aged65To74_Owner = \"H13_009N\",Aged65To74_Renter = \"H13_019N\",  Aged75To84_Owner = \"H13_010N\", Aged75To84_Renter = \"H13_020N\",Aged85Over_Owner = \"H13_011N\", Aged85Over_Owner = \"H13_021N\"),\n  state = \"IA\",\n  county = \"Polk\",\n  year = 2020,\n  geometry = TRUE,\n  sumfile = \"dhc\"\n)\n\n\nmapview(population65AndAbove, \n                          zcol = \"value\",\n                          layer.name = \"Aged 65 and Above\", \n                          cex = \"value\",  # Use the population value to scale the size of the dots\n                          alpha = 0.7, \n                          legend = TRUE)\n\n 3. Navigate through GitHub\n-   Learning GitHub skills through interactive courses (Pull request, Commit, Create a new repository)\n-   Transition into collaborative group work with team members using GitHub\n\n![](imgs/GitHub.png){width=\"399\"}\n\nImage Preprocessing through Python\n\nThresholding (Converting grayscale images into binary images for better analysis)\nEdge Detection through Sobel and Canny\nGaussian Smoothing (Blur image and reduce contrast)\nImage Transformation (Rotating, Rescaling, Aliasing, Resizing)\nFinding Contours\n\n\n\nAfter going through a datacamp course on image processing, I have the chance to put that into practice by working on extracting features from a Sanborn Map. Firstly, I load the image and convert the image to grayscale for further processing. I will then use Gaussian filtering to reduce constrast of the image and apply thresholding. To extract the buildings from the image, I applied the edge detection technique using Canny and apply Contours on the edge of the buildings.\n\n\n \n\n\nllamaindex\n\n\nLlamaindex is a data framework for connecting custom data sources to large language models (LLMs). The idea is to incorporate Llamaindex in the Housing and AI project where a user ask a query and the model is able to perform intelligent searching of data and provide a suitable response.\nAfter taking several tutorials on Llamaindex, I have a clearer understanding on the implementation and flow of the system as shown below:\n\n\n\n\nLlamaindex flow chart\n\n\n\nAlso, I had the opportunity to implement Llamaindex on both the structured and unstructured datasets for the Housing project. While working with the unstructured data, I was able to implement both the Vector Query Engine and the Summary Query Engine, and using a Router Query Engine where it will choose the suitable query engine depending on the query. Some of the codes and the results are shown below:\n\n\n\n\n\nWhile dealing with structured data such as a CSV data file, I implemented a Pandas Query Engine where it is able to take in a query from the user, perform pandas operation and provide an output as shown below:"
  },
  {
    "objectID": "studentBlogs/alister/student-midpoint-blog-Alister03USA/Alister_Midpoint_Blog.html#new-learning-direction",
    "href": "studentBlogs/alister/student-midpoint-blog-Alister03USA/Alister_Midpoint_Blog.html#new-learning-direction",
    "title": "Alister’s Midpoint Blog",
    "section": "New Learning Direction",
    "text": "New Learning Direction\n\nDuring the program, I’ve pursued new directions in learning and not being afraid to step out of my comfort zone in pursuing new tools such as image processing and Llamaindex. Also, I have understand the importance to reading and understanding the documentation, something that is often overlooked. Besides, one major learning curve from the program is being accountable of your own work and learning to explore the abundance of online resources or seek help from others whenever encountering any problems."
  },
  {
    "objectID": "studentBlogs/alister/student-midpoint-blog-Alister03USA/Alister_Midpoint_Blog.html#skills-to-improve-for-the-rest-of-the-program",
    "href": "studentBlogs/alister/student-midpoint-blog-Alister03USA/Alister_Midpoint_Blog.html#skills-to-improve-for-the-rest-of-the-program",
    "title": "Alister’s Midpoint Blog",
    "section": "Skills To Improve For The Rest Of The Program",
    "text": "Skills To Improve For The Rest Of The Program\n\nContinue to improve using R for data analysis and visualization especially in plotting maps\nFamiliarize with the Llamaindex documentation and able to apply advanced concepts and customization for my query system\nConvey the data findings and interpretations in a more accurate and concise way\nActively participate and contribute ideas during meetings"
  },
  {
    "objectID": "studentBlogs/alister/student-midpoint-blog-Alister03USA/Alister_Midpoint_Blog.html#goals-outline",
    "href": "studentBlogs/alister/student-midpoint-blog-Alister03USA/Alister_Midpoint_Blog.html#goals-outline",
    "title": "Alister’s Midpoint Blog",
    "section": "Goals Outline",
    "text": "Goals Outline\n\nTruth be told, DSPG has exceeded my expectations during these 5 weeks. I felt that I have grown so much and the learning progress has given me a clearer idea on my career pathway as a data scientist. Reflecting on my initial goals, I believe I am on the right track as the program has significantly advanced my technical capabilities and provided a solid foundation in data science practices. After a few weeks into the program, I found myself gaining valuable first hand experience into the job scope of a data scientist, and the opportunity to meet with clients and experts in different fields helped prepare me for the working world. While there is always room for improvements, I am confident that the skills and experiences gained from the program will greatly benefit me in the long run."
  },
  {
    "objectID": "studentBlogs/manjul/student-midpoint-blog-ManjulBalayar/Manjul_Midpoint_Blog.html",
    "href": "studentBlogs/manjul/student-midpoint-blog-ManjulBalayar/Manjul_Midpoint_Blog.html",
    "title": "Manjul’s Midpoint Reflection",
    "section": "",
    "text": "Throughout this program, by having the opportunity to participate in different activities while simultaneously also working on our project specific work, I have been able to to improve and learn new a variety of new skills. This blog focuses of the skills I have improved on and skills I hope to get better at in the future."
  },
  {
    "objectID": "studentBlogs/manjul/student-midpoint-blog-ManjulBalayar/Manjul_Midpoint_Blog.html#skills-i-have-improved-on",
    "href": "studentBlogs/manjul/student-midpoint-blog-ManjulBalayar/Manjul_Midpoint_Blog.html#skills-i-have-improved-on",
    "title": "Manjul’s Midpoint Reflection",
    "section": "Skills I have improved on",
    "text": "Skills I have improved on\n\nPython\nAs a software engineering student, we don’t use Python in our courses so I’ve had to learn Python myself and haven’t had the opportunity to grow my skills in this language through my classes. Past few weeks I’ve been using Python a lot for data pre-processing and visualizations.\n\n\nData Pre-processing & Visualization w/Python\nI have utilized Python libraries such as Pandas, Numpy, Scipy, Sklearn, Matplotlib, Seaborn etc, for my data pre-processing and visualization tasks. I have done some relational database using MySQL but never with Python, so it was interesting to merge and join dataframes in Python.\n\n\n\nGitHub\nI have worked with Git before for my project based courses, but it has some time since I have used GitHub with multiple people, merging, resolving conflicts, creating pull requests, etc. I’ve been forgetting some git terminal commands but GitHub Desktop just makes life so easy, thanks Harun for putting me on!\nLook at all those repos!"
  },
  {
    "objectID": "studentBlogs/manjul/student-midpoint-blog-ManjulBalayar/Manjul_Midpoint_Blog.html#skills-that-are-new-to-me",
    "href": "studentBlogs/manjul/student-midpoint-blog-ManjulBalayar/Manjul_Midpoint_Blog.html#skills-that-are-new-to-me",
    "title": "Manjul’s Midpoint Reflection",
    "section": "Skills that are new to me:",
    "text": "Skills that are new to me:\nAlong with refreshing on past skills, I have also had the opportunity to learn new skills and concepts.\n\nR and it’s libraries\nAlthough I’ve used mostly Python for handling datasets, but I plan to use R for my visualizations. I have to admit, it’s better for visualizations. I’ve taken multiple DataCamp courses as well for R which focused on R visualizations so they’ve been quite helpful. The activities where we learned about Census data, using ‘tidycensus’ and other R libraries(ggplot, leaflet, tigris, mapview, dplyr, etc…) have been beneficial to our project specific work as well. We have worked with ACS and other government data since then and those experiences we gained from those activities were helpful.\nHere is a interactive map using leaflet and tigris which shows the economic connectedness of a county in Iowa.\n\n\n\nQuarto Web App\nI think Quarto is really unique and pretty cool. I was skeptical at first but it’s a great system to create blogs have showcase your work you’ve done so far and it allows multiple languages to be used in the same page. Sometimes it’s a pain to make everything work but it’s unique and helpful especially when you have a group that uses different languages and libraries.\n\n\nProject specific: Community Capitals & Events Database\nFor our project we are focusing on two different projects, but they are correlated and the end goal is to showcase both of them in our final website. Past few weeks, I’ve had the opportunity to understand what each of the community capitals are, finding ways to measure each capital, finding reliable data sources for them, and A LOT OF DATA-PREPROCESSING for both capitals and events project.\nIf I had to sum up what I’ve been doing so far, find data source if not given —&gt; extract raw data —&gt; clean and reshape raw data into something useful but also make sure it’s format align with our other dataset so we can use it later for our final visualization website…"
  },
  {
    "objectID": "studentBlogs/manjul/student-midpoint-blog-ManjulBalayar/Manjul_Midpoint_Blog.html#skills-i-hope-to-improve",
    "href": "studentBlogs/manjul/student-midpoint-blog-ManjulBalayar/Manjul_Midpoint_Blog.html#skills-i-hope-to-improve",
    "title": "Manjul’s Midpoint Reflection",
    "section": "Skills I hope to improve",
    "text": "Skills I hope to improve\n\nData visualization in R\nSo far I’ve mostly worked with the making usable datasets and haven’t tapped into much data visualization and exploratory aspects. This is something we will be doing a lot in the next few weeks so it’s a skill I hope to become proficient in.\n\n\nPlaying around with raw datasets, cleaning, reshaping, etc\nAlthough I have been doing this, there are still many things I have to learn. I often rely a lot on my projects leader and graduate fellow for making decisions on the dataset, and I also tend to rely on ChatGPT. Therefore, I hope to independent and grow my skills in a important aspect of Data Science.\n\n\nMachine Learning Techniques\nI’m not sure how much ML techniques we will be applying in the future for our projects, but since it’s a field of interest for me, I hope to gain more experience in this field. I’ve applied few ML techniques like scaling features with MinMaxScaler before in our project specific work, they’re very helpful and have many powerful libraries that could be beneficial. Although we may not build large language models or powerful CV models, I hope we apply some fundamental techniques in the future.\nHere’s some code chunk of using feature transformation and scaling techniques in our of our datasets:\n\n# Ensure there are no zero or negative values for transformations that require positivity\nshift = 1 - new_merged['total_amount'].min()\n\n# Applying transformations\nnew_merged['log_total_amount'] = np.log(new_merged['total_amount'] + shift)\nnew_merged['sqrt_total_amount'] = np.sqrt(new_merged['total_amount'])\n_, best_lambda = boxcox(new_merged['total_amount'] + shift)\nnew_merged['boxcox_total_amount'] = boxcox1p(new_merged['total_amount'], best_lambda)\n\n# Scaling transformations\nscaler = MinMaxScaler()\nnew_merged['log_event_magnitude'] = scaler.fit_transform(new_merged[['log_total_amount']])\nnew_merged['sqrt_event_magnitude'] = scaler.fit_transform(new_merged[['sqrt_total_amount']])\nnew_merged['boxcox_event_magnitude'] = scaler.fit_transform(new_merged[['boxcox_total_amount']])\n\n# Formatting scaled values\nfor col in ['log_event_magnitude', 'sqrt_event_magnitude', 'boxcox_event_magnitude']:\n    new_merged[col] = new_merged[col].apply(lambda x: \"{:.2f}\".format(x))\n\nFrom our first week at Durham to now, a month went by really quick and it’s hard to believe we are already halfway. But my time here so far has been very insightful and I am excited to see what experiences I will continue to gain during the second half!"
  },
  {
    "objectID": "studentBlogs/lily/student-introductions/Lily_Introduction.html",
    "href": "studentBlogs/lily/student-introductions/Lily_Introduction.html",
    "title": "Lily’s Introduction",
    "section": "",
    "text": "My name is Lily, and I am from Des Moines, Iowa. In fall 2024, I will be a senior majoring in statistics and minoring in general business. I chose a statistics major because I have always enjoyed math, particularly statistics. I chose to minor in business because I believe it would be useful when working with companies.\n\n\n\n\n\n\n\n\n\n\nMy plans are to get a job in data analytics/data science. Data science is important because it gives people and businesses valuable information and insight about data sets. Which in turn helps with decision-making.\n\n\n\n\n\nI have some prior data science knowledge from my previous statistics classes. I have learned how to analyze and make visuals from datasets using R. I also learned how to produce statistical outputs to help answer questions.\n\n\n\n\nSummary of Adverstising data\n\n\n\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n\n\nTV\n1.0\n49.75\n97.50\n96.140\n143.25\n190\n\n\nNewspaper\n0.3\n12.75\n25.75\n30.554\n45.10\n114\n\n\nSales\n1.0\n31.75\n55.00\n58.370\n85.00\n121\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome extracurricular activities I enjoy are going to the gym, enjoying nature, playing with my dog, photography, watching movies and sports, and spending time with my friends and family. An interesting fact about me is that I can play 4 instruments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkills that I value and appreciate about myself are that I am a very hard worker. When things get difficult, I don’t give up and try multiple ways to solve the issue. I have good communication, collaboration, and listening skills.\n\n\n\n\n\nThree skills I want to develop throughout DSPG are my coding and data analytics skills. I also want to learn how to use SQL and strengthen my skills in Python, and R. Some of my data science skills are analyzing statistical outputs and using R studio to make visualizations. I also am curious and believe that comes in handy with data science.\n\n\n\n\n\nI am pursuing these skills because I believe data science is important work. I will become a better data scientist and strengthen my analytical skills. With these skills I will be able to use them in future jobs. It will give me the skills and knowledge needed to create insights for data.\n\n\n\n\n\nI plan to improve these skills by taking courses on datacamp to strengthen my knowledge in certain software. I also will work with my peers to work through problems. I also can utilize LinkedIn Learnings to learn more about different software."
  },
  {
    "objectID": "studentBlogs/lily/student-introductions/Lily_Introduction.html#about-me",
    "href": "studentBlogs/lily/student-introductions/Lily_Introduction.html#about-me",
    "title": "Lily’s Introduction",
    "section": "",
    "text": "My name is Lily, and I am from Des Moines, Iowa. In fall 2024, I will be a senior majoring in statistics and minoring in general business. I chose a statistics major because I have always enjoyed math, particularly statistics. I chose to minor in business because I believe it would be useful when working with companies.\n\n\n\n\n\n\n\n\n\n\nMy plans are to get a job in data analytics/data science. Data science is important because it gives people and businesses valuable information and insight about data sets. Which in turn helps with decision-making.\n\n\n\n\n\nI have some prior data science knowledge from my previous statistics classes. I have learned how to analyze and make visuals from datasets using R. I also learned how to produce statistical outputs to help answer questions.\n\n\n\n\nSummary of Adverstising data\n\n\n\nMin.\n1st Qu.\nMedian\nMean\n3rd Qu.\nMax.\n\n\n\n\nTV\n1.0\n49.75\n97.50\n96.140\n143.25\n190\n\n\nNewspaper\n0.3\n12.75\n25.75\n30.554\n45.10\n114\n\n\nSales\n1.0\n31.75\n55.00\n58.370\n85.00\n121\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome extracurricular activities I enjoy are going to the gym, enjoying nature, playing with my dog, photography, watching movies and sports, and spending time with my friends and family. An interesting fact about me is that I can play 4 instruments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkills that I value and appreciate about myself are that I am a very hard worker. When things get difficult, I don’t give up and try multiple ways to solve the issue. I have good communication, collaboration, and listening skills.\n\n\n\n\n\nThree skills I want to develop throughout DSPG are my coding and data analytics skills. I also want to learn how to use SQL and strengthen my skills in Python, and R. Some of my data science skills are analyzing statistical outputs and using R studio to make visualizations. I also am curious and believe that comes in handy with data science.\n\n\n\n\n\nI am pursuing these skills because I believe data science is important work. I will become a better data scientist and strengthen my analytical skills. With these skills I will be able to use them in future jobs. It will give me the skills and knowledge needed to create insights for data.\n\n\n\n\n\nI plan to improve these skills by taking courses on datacamp to strengthen my knowledge in certain software. I also will work with my peers to work through problems. I also can utilize LinkedIn Learnings to learn more about different software."
  },
  {
    "objectID": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-housing-ai/Weekly_Team_Blog.html",
    "href": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-housing-ai/Weekly_Team_Blog.html",
    "title": "Housing and AI Week Two",
    "section": "",
    "text": "This week we continued looking for data for the smallest unit of area we could.\nOn the side of population density, we were able to find data down to the house/parcel level for Des Moines and we were able to go ahead and display them along with the census block groups and all residential zones in the Des Moines area as shown in the figure below. We can do the same with the blocks as well.\n\nFrom here we wanted to see if we could find data of the same variety for smaller communities rather than just larger cities so our eventual model isn’t biased. Luckily, the Des Moines Data Portal stretches across the entire Des Moines Metropolitan Area which meant we could get data for smaller communities such as Ackworth that has a population of 115.\n\n\nHowever, we realized we don’t just want communities around the Des Moines Metro and still wanted to reach smaller community data. After some research we found that Iowa has a free GIS database (https://www.iowagisdata.org/) that contains information for numerous counties and cities down to the parcel and building level. One county that we found may be helpful is Winneshiek County as it is in a more rural area of the state away from most metro and micropolitan areas.\n\nThe image above is showing all residential buildings in Decorah, but the database has access to so many more communities and levels of data that will continued to be explored.\nWe were also able to get a way to calculate population density of our blocks and block groups using tidycensus.\n\nWe also wanted to look into transportation data, however outside of getting routes and stops we couldn’t find much. Additionally, after talking to Omar Padilla this makes sense as most people just don’t prefer to use public transit compared to their own car in Iowa even in the bigger cities like Des Moines which is why much data isn’t available"
  },
  {
    "objectID": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-housing-ai/Weekly_Team_Blog.html#data-exploration-for-population-density",
    "href": "blogs2024/Team_Blog_Two/dspg-weekly-team-blog-two-housing-ai/Weekly_Team_Blog.html#data-exploration-for-population-density",
    "title": "Housing and AI Week Two",
    "section": "",
    "text": "This week we continued looking for data for the smallest unit of area we could.\nOn the side of population density, we were able to find data down to the house/parcel level for Des Moines and we were able to go ahead and display them along with the census block groups and all residential zones in the Des Moines area as shown in the figure below. We can do the same with the blocks as well.\n\nFrom here we wanted to see if we could find data of the same variety for smaller communities rather than just larger cities so our eventual model isn’t biased. Luckily, the Des Moines Data Portal stretches across the entire Des Moines Metropolitan Area which meant we could get data for smaller communities such as Ackworth that has a population of 115.\n\n\nHowever, we realized we don’t just want communities around the Des Moines Metro and still wanted to reach smaller community data. After some research we found that Iowa has a free GIS database (https://www.iowagisdata.org/) that contains information for numerous counties and cities down to the parcel and building level. One county that we found may be helpful is Winneshiek County as it is in a more rural area of the state away from most metro and micropolitan areas.\n\nThe image above is showing all residential buildings in Decorah, but the database has access to so many more communities and levels of data that will continued to be explored.\nWe were also able to get a way to calculate population density of our blocks and block groups using tidycensus.\n\nWe also wanted to look into transportation data, however outside of getting routes and stops we couldn’t find much. Additionally, after talking to Omar Padilla this makes sense as most people just don’t prefer to use public transit compared to their own car in Iowa even in the bigger cities like Des Moines which is why much data isn’t available"
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html",
    "title": "Housing and AI Week Three",
    "section": "",
    "text": "At the start of this week we were able to get a better dataset for the residential parcels in Polk county that does a better job of depicting which parcels actually contain housing units and how many units there actually are. It also helps divide the types of builds better ranging from single-family homes to apartments to nursing homes.\n\n\n\n\n\n\n\n\n\n\nHomes\n\n\n\n\n\n\n\nCondos\n\n\n\n\n\nThe other major project we are focusing on is parcel extraction/segmentation using the Prithvi model. The idea is that Prithvi is an object identification model that was trained on NASA’s satellite HSL data and then uses that to segment various objects. The good thing is that the “backbone” of this model can be downloaded and is pretrained. What we need to do is create a head and neck that can take our images and data that we have been gathering and then embed the image so it can be fed into the model and then turn it into a segmented mask that we can overlay on top of our images with the predicted parcels. This is so we can get around having to get every county’s assessor data and being able to apply it also for places that lack parcel level data.\nSome work that has been done that we can base our model on comes in regards to flood detection on a satellite image."
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html#tax-parcel-and-model-extractionsegmentation",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html#tax-parcel-and-model-extractionsegmentation",
    "title": "Housing and AI Week Three",
    "section": "",
    "text": "At the start of this week we were able to get a better dataset for the residential parcels in Polk county that does a better job of depicting which parcels actually contain housing units and how many units there actually are. It also helps divide the types of builds better ranging from single-family homes to apartments to nursing homes.\n\n\n\n\n\n\n\n\n\n\nHomes\n\n\n\n\n\n\n\nCondos\n\n\n\n\n\nThe other major project we are focusing on is parcel extraction/segmentation using the Prithvi model. The idea is that Prithvi is an object identification model that was trained on NASA’s satellite HSL data and then uses that to segment various objects. The good thing is that the “backbone” of this model can be downloaded and is pretrained. What we need to do is create a head and neck that can take our images and data that we have been gathering and then embed the image so it can be fed into the model and then turn it into a segmented mask that we can overlay on top of our images with the predicted parcels. This is so we can get around having to get every county’s assessor data and being able to apply it also for places that lack parcel level data.\nSome work that has been done that we can base our model on comes in regards to flood detection on a satellite image."
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html#more-data-visualization-using-leaflet",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html#more-data-visualization-using-leaflet",
    "title": "Housing and AI Week Three",
    "section": "More Data Visualization using leaflet",
    "text": "More Data Visualization using leaflet\n\nThis week, I worked more with Leaflet maps and tried different things. First, I made a map using block group level data and it was easy because I got the geojason format data from ACS.\nThen, I tried something harder: making maps for streets and small blocks. For the streets, I got data from HUD and used a tool called “Geocodio” to find their exact locations.\nFor small blocks, I looked at where majority of senior age groups live in Des Moines. I used something called Tiger Line to get the Geographic data, I converted the shp file to csv, and I only looked at Des Moines to keep it simple. Then, I combined all this data and used a tool called GeoPandas to organize it better and turn it into a type of map file called GeoJSON.\nNext week, I want to make one function that does all this work. This means when someone gives it data, it will automatically make a map for them. This will make everything much easier and quicker.\n\n\n\nStreet level map for Low-Income Housing Tax Credit data\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nSeniors Age Distributation map in Des Moines"
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html#llamaindex",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html#llamaindex",
    "title": "Housing and AI Week Three",
    "section": "Llamaindex",
    "text": "Llamaindex\n\nRouter Query Engine\n\nA custom query engine that select one of the several query engine candidates\n\nStep 1: Get the nodes (chunks of the document) and index them\n\nStep 2: Define query engines, and wrap them around the tools\nStep 3: Object index allows us to use our index data structures over objects, passed in to our agent for tool retrieving during query time\nResults:\n\n\n\nUsing vector query engine\n\n\n\n\n\nUsing summary query engine\n\n\n\n\nQuery Pipeline For Text To Pandas\n\nDealing with structured data (.csv, .xlx, etc)"
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html#to-be-continued",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html#to-be-continued",
    "title": "Housing and AI Week Three",
    "section": "To be Continued:",
    "text": "To be Continued:\n\nApplying Prompt Template (Instruction to be parsed) to be Query Pipeline to make sure the dataframe is correctly formatted\nIntegrate a function calling tool in the query pipeline to call the mapping function which in return will output a map of Iowa to showcase the attributes based on the query of the user"
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html#sources",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-housing-ai/Weekly_Team_Blog.html#sources",
    "title": "Housing and AI Week Three",
    "section": "Sources",
    "text": "Sources\n\nHugging Face Prithvi_100M: Hugging Face Team. (n.d.). IBM-NASA-geospatial/prithvi-100m · hugging face. ibm-nasa-geospatial/Prithvi-100M · Hugging Face. https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M"
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-community-capitals-1/Weekly_Team_Blog.html",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-community-capitals-1/Weekly_Team_Blog.html",
    "title": "Community Capitals Week Three",
    "section": "",
    "text": "This week we continued our work in the Community Capitals project and we are specifically still looking into the social and cultural capital measurements and datasets. Our goal this week was to transform raw datasets we collected for the measurements we came up with and transformed them so that we can use it as one of our indicators in the final data visualization web app. We then also looked at couple of other datasets that were already finalized by Solomon and we try to do some data exploratory and visualizations with them."
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-community-capitals-1/Weekly_Team_Blog.html#week-goals",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-community-capitals-1/Weekly_Team_Blog.html#week-goals",
    "title": "Community Capitals Week Three",
    "section": "",
    "text": "This week we continued our work in the Community Capitals project and we are specifically still looking into the social and cultural capital measurements and datasets. Our goal this week was to transform raw datasets we collected for the measurements we came up with and transformed them so that we can use it as one of our indicators in the final data visualization web app. We then also looked at couple of other datasets that were already finalized by Solomon and we try to do some data exploratory and visualizations with them."
  },
  {
    "objectID": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-community-capitals-1/Weekly_Team_Blog.html#social-and-cultural-capital-datasets",
    "href": "blogs2024/Team_Blog_Three/dspg-weekly-team-blog-three-community-capitals-1/Weekly_Team_Blog.html#social-and-cultural-capital-datasets",
    "title": "Community Capitals Week Three",
    "section": "Social and Cultural Capital Datasets",
    "text": "Social and Cultural Capital Datasets\n\nFBI Crime Reports\nWe worked with the FBI Crime Reports dataset which consisted all the crimes reported in the year of 2021 for all offenses. There are a total of fifty different offenses in the dataset, but we primarily focused on four which were classified as the most violent crimes:\n\nMurder: 11(Non-negligent)\nForcible Rape: 20\nRobbery: 30\nAggravated Assault: 40\n\nWe believed that these four in particular could be a strong measure for social capital. High rates of such crimes might indicate low levels of social cohesion, trust, and mutual aid among community members, which are key components of social capital.\nThe dataset initially consisted over 78 columns, with a lot of redundant columns but also followed a ‘wide’ format which was very hard to read, especially when trying to see the demographics on who committed the crime.\n\nWe then transformed this from a wide format to a long format, and we kept the total crime count for each offense in each county, as well as the crime count per 10k population. This way we have much less columns, and we can extract important crime information for each offense in each county much easier.\n For most of our datasets, we are trying to follow a format where in each county level dataset we have all the 99 different counties in the ‘COUNTY’ column, this way, it is much easier when merging datasets for our final data visualization.\n\n\n\nSocial Capital Atlas\nAlong with the FBI Crime Reports dataset, we looked into Meta’s Social Capital Atlas dataset, a very cool website that shows the social engagement of a community on their social media platform.\nDifferent measures for social capital in this dataset:\n\nConnectedness: How people with different characteristics and backgrounds are friends with each other, this is a clear example of bridging social capital.\nCohesiveness: The degree to which friendship networks are clustered into cliques and whether friendships tend to be supported by mutual friends. This includes our clustering and support ratio measures.\nCivic Engagement: Indices of trust or participation in civic organizations. This includes our volunteering rate measure.\n\n\nEconomic Connectedness by County\n\nNext week goal: We have the same dataset but by zip code instead and we would like to generate maps for each variable by zip code.\n\n\nIowa Public Library Statistics\nWe looked at the Iowa Public Library Statistics for the fiscal year 2023, July 1, 2022 - Jun 30, 2023. There were 514 Libraries in the dataset and each library was set to a size range. The main sections that we looked at were expenditures, programs and activities, services, and transactions. This will tell us how involved a community is to their public library.\n\n\nProgram and Activities\n\n\n    City  Pop Size Prog1 Attend1 Prog2 Attend2 Prog3 Attend3 Prog4 Attend4 Prog5 Attend5 Prog6 Attend6 Num Programs per 10,000 People Num of Attend per Pop Total Kid Programs Num Kid Programs per 10,000 People Total Kid Programs Attend Num of Kid Programs Attend per 10,000 People\n1 Ackley 1699    C     5      28    46    1106     0       0    68     379    12     441   131    1954                       771.0418             1.1500883                 51                          300.17657                      1134                                     6674.514\n2  Adair  828    B     0       0     5     245     0       0     1      10     6     150    12     405                       144.9275             0.4891304                  5                           60.38647                       245                                     2958.937\n3   Adel 6090    E   143    2638    37     719    33     397   327    1902    35    1635   575    7291                       944.1708             1.1972085                180                          295.56650                      3357                                     5512.315\n4 Agency  463    B     0       0    11      56     0       0    12      40     0       0    23      96                       496.7603             0.2073434                 11                          237.58099                        56                                     1209.503\n5  Akron 1580    C    40     980    32    1353     4      72    55     407    25     878   156    3690                       987.3418             2.3354430                 72                          455.69620                      2333                                    14765.823\n\n\nIn this table we added columns to find out the number of programs and attendance per 10,000 people in each city. We also looked at just the kid programs. In this dataset kids are classified by the age range 0-5 and 6-11.\n\n\nServices\n\n\n    City  Pop Size Visits Visits Per Capita Internet PCs Internet Use Wireless Sessions Website Visits St. Ft. of Building Avg. Weekly Hours Open Visits Per 10,000 People Wireless Sessions Per 10,000 People Website Visits Per 10,000 People\n1 Ackley 1699    C  11000          6.474397            4          520              4183           2203                5300                     42                 64743.97                           24620.365                        12966.451\n2  Adair  828    B   1582          1.910628            2          146               319            290                3200                     13                 19106.28                            3852.657                         3502.415\n3   Adel 6090    E  62001         10.180788            7         1543             11756          22079               18000                     43                101807.88                           19303.777                        36254.516\n4 Agency  463    B   1716          3.706263            4           13               966            612                 357                     43                 37062.63                           20863.931                        13218.143\n5  Akron 1580    C  18342         11.608861            5          802              2076           2747                2412                     43                116088.61                           13139.241                        17386.076\n\n\nIn this table we were interested in the services that a library had for their community. We calculated the visits, wireless sessions, and website visits per 10,000 people.\n\n\n\n2022 General Election Turnout\nIn the General Election Turnout dataset we were given information about election day, absentee, and active voters in Iowa counties.\n\nElection Turnout\n\n\n     County   Pop Election Day Voters Absentee Voters Total Voters Active Voters as of 11/8/2022 % Active Voter Turnout Inactive Voters as of 11/8/2022 % Total Voter Turnout % Total Absentee Voters Total Voters per 10,000 people Active Voters per 10,000 people Election Day Voters per 10,000 people Absentee Voters per 10,000 people\n1     Adair  7479                2249             943         3192                          4707                 0.6781                             719                0.5882               0.2954261                       4267.950                        6293.622                              3007.087                          1260.864\n2     Adams  3680                1142             538         1680                          2446                 0.6868                             378                0.5949               0.3202381                       4565.217                        6646.739                              3103.261                          1461.957\n3 Allamakee 14046                3935            1915         5850                          8423                 0.6945                            1424                0.5940               0.3273504                       4164.887                        5996.725                              2801.509                          1363.377\n4 Appanoose 12279                3416            1319         4735                          7434                 0.6369                            1450                0.5329               0.2785639                       3856.177                        6054.239                              2781.986                          1074.192\n5   Audubon  5651                1746             739         2485                          3691                 0.6732                             529                0.5888               0.2973843                       4397.452                        6531.587                              3089.719                          1307.733\n\n\nIn this table we calculated the total amount of voters per 10,000 people in each county. We also calculated the election day, absentee, and active voters per 10,000 people in each county.\n\n\n\nExploration of Arts, Events, and Race\nWe started to explore the dataset about art, events, and race in Iowa cities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two plots show you the relationship between the city population and the Historic Sites that are less than 100.\n\nMap Exploration\n\nThis interactive map shows cities in Iowa with less than 100 historic sites per 10,000 people.\n![](imgs/Public_art_less.html{width=“600” height=“400”}\nThis interactive map shows cities in Iowa with less than 100 public art per 10,000 people.\n\nRace\n\n\n\nAncestry\n\n\n\nArt Sites\n\n\n\nHistoric Sites\n\n\n\nMuseums\n\nThis image shows 100 cities in Iowa with the highest race Simpson Index.\nThese treemaps show 100 cities in Iowa that have the highest Simpson Index race and ancestry, art sites, historic sites, and museums.\n\n\n\n\n3 ACS Datasets\n\nSingle Household Distribution across Cities\nWe started by collecting single household datasets from ACS\n\n\n    GEOID                NAME B11003_010E B11003_010M B11003_016E B11003_016M\n1 1900190   Ackley city, Iowa          10           9          41          28\n2 1900235 Ackworth city, Iowa           0          10           8           8\n3 1900370    Adair city, Iowa           0          10           8           9\n4 1900505     Adel city, Iowa           0          15          61          73\n5 1900595    Afton city, Iowa           1           3          15          12\n6 1900640   Agency city, Iowa           0          10          22          24\n\n\nBecause of population differences between cities, we fetched the total number of households within cities from ACS and calculated proportions of single households in cities.\n\n\n    GEOID     CITY B11003_010E B11003_010M B11003_016E B11003_016M estimate proportion_male proportion_female\n1 1900190   Ackley          10           9          41          28      413     0.024213075        0.09927361\n2 1900235 Ackworth           0          10           8           8       32     0.000000000        0.25000000\n3 1900370    Adair           0          10           8           9      185     0.000000000        0.04324324\n4 1900505     Adel           0          15          61          73     1525     0.000000000        0.04000000\n5 1900595    Afton           1           3          15          12      245     0.004081633        0.06122449\n6 1900640   Agency           0          10          22          24      135     0.000000000        0.16296296\n\n\n\n\nCommuting Time to Work\nFrom a bunch of datasets for means of transportations from ACS, we collected the dataset of people commuting to work over one hour every day.\n\n\n    GEOID                NAME   variable num_commuters moe\n1 1900190   Ackley city, Iowa B08134_010            29  21\n2 1900235 Ackworth city, Iowa B08134_010             0  10\n3 1900370    Adair city, Iowa B08134_010            34  24\n4 1900505     Adel city, Iowa B08134_010            23  35\n5 1900595    Afton city, Iowa B08134_010            35  24\n6 1900640   Agency city, Iowa B08134_010             2   4\n\n\nAlso because of the size differences, I collectd the total number of communters and then calculated the proportion of people commuting more than 1 hour to work.\n\n\n    GEOID     CITY   variable num_commuters moe estimate  proportion\n1 1900190   Ackley B08134_010            29  21      752 0.038563830\n2 1900235 Ackworth B08134_010             0  10       50 0.000000000\n3 1900370    Adair B08134_010            34  24      390 0.087179487\n4 1900505     Adel B08134_010            23  35     2674 0.008601346\n5 1900595    Afton B08134_010            35  24      500 0.070000000\n6 1900640   Agency B08134_010             2   4      199 0.010050251\n\n\n\n\nEnglish Proficiency Dataset\nEnglish proficiency also contributes to how society is established, expresses and communicates ideas.\n Also, to maintain good relativity to the city sizes, I calculate the proportions.\n\n\n      CITY proportion\n1   Ackley  0.9660912\n2 Ackworth  1.0000000\n3    Adair  1.0000000\n4     Adel  0.9799023\n5    Afton  0.9630332\n6   Agency  1.0000000\n\n\n\n\n\nMerge Datasets together\nThese three datasets will be valuable to measurer the social capitals at the end and we just care about the proportions. So I perform a merge.\n\n\n\nSocial Capital Measures\n\n\n\n\nRace & Ancestry Diversity Dataset\nRace and Ancestry are important measures for Cultural Capitals. I collected data about number of people with different races and ancestries within cities and merge them together.\n\n\n\nAncestry\n\n\n We then use Simpson Index to calculate the diversity within each city.\n\n\n    GEOID     CITY SimpsonIndex_race SimpsonIndex_ancestry\n1 1900190   Ackley        0.24674401             0.8196178\n2 1900235 Ackworth        0.17250674             0.8134410\n3 1900370    Adair        0.09779811             0.7921790\n4 1900505     Adel        0.17635778             0.8601441\n5 1900595    Afton        0.26618045             0.8464086\n6 1900640   Agency        0.08874089             0.8409283\n\n\n\n\nCultural Datasets\nWe process to some visualizations for some of the measures for cultural capitals\n\n\n\nCultural\n\n\n\nVisualizations\n\nHistoric Sites\n\n\n\nArts\n\n\n\nMuseums\n\n\n\nMonuments"
  }
]